{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-11T01:12:32.544505800Z",
     "start_time": "2023-12-11T01:12:29.417200200Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [1], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01moptim\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchtext\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Multi30k\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchtext\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Field, BucketIterator\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c9d7979107b69",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78d06c7f266dbef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T21:45:14.191791500Z",
     "start_time": "2023-12-09T21:45:14.133792500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8879b5a840b7f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30a06d932a5f623e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T21:45:14.192790Z",
     "start_time": "2023-12-09T21:45:14.152801Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, dropout_probability):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_layer = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout_probability)\n",
    "\n",
    "        # Dropout layer to prevent over fitting (regularization)\n",
    "        # it randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.\n",
    "        self.dropout = nn.Dropout(dropout_probability)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # inputs = [inputs len, batch size]\n",
    "        dropout_embeddings = self.dropout(embeddings)\n",
    "\n",
    "        # embedded = [inputs len, batch size, emb dim]\n",
    "        outputs, (hidden, cell) = self.lstm_layer(dropout_embeddings)\n",
    "\n",
    "        # outputs = [inputs len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        # outputs are always from the top hidden layer\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467cd3fa6b68bdc2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125423347cfa284b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
