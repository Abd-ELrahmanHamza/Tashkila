{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:10:14.646511700Z",
     "start_time": "2023-12-30T18:10:14.576809800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "DATASET_FOLDER = 'dataset'\n",
    "CLEAN_OUT_FOLDER = 'clean_out'\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:10:16.169512Z",
     "start_time": "2023-12-30T18:10:14.588809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:  (50000,)\n",
      "first 5 examples: \n",
      "0    قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَ...\n",
      "1    ابْنُ عَرَفَةَ : قَوْلُهُ : بِلَفْظٍ يَقْتَضِي...\n",
      "2    ( قَوْلُهُ لِعَدَمِ مَا تَتَعَلَّقُ إلَخْ ) أَ...\n",
      "3                       وَحَيَوَانٌ غَيْرُ مَوْجُودٍ .\n",
      "4    فَائِدَةٌ : قَالَ بَعْضُهُمْ : يُؤْخَذُ مِنْ ش...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# read dataset train \n",
    "def read_dataset(file_name = 'train.txt' ,verbose=False):\n",
    "    \"\"\"\n",
    "    takes a file name and returns a pandas dataframe of the dataset \n",
    "    the returnes is a pandas series of the dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = os.path.join(DATASET_FOLDER, file_name)\n",
    "    # read dataset as a text file and each line as a training example\n",
    "    dataset = pd.read_csv(dataset, sep='\\t', header=None).squeeze('columns')\n",
    "    \n",
    "    if verbose:\n",
    "        print('dataset shape: ', dataset.shape)\n",
    "        print('first 5 examples: ')\n",
    "        print(dataset[:5])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "read_dataset(verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:10:16.181506300Z",
     "start_time": "2023-12-30T18:10:16.169512Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:10:16.183586400Z",
     "start_time": "2023-12-30T18:10:16.177509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "ت 1578\n",
      "َ 1614\n",
      "غ 1594\n",
      "ْ 1618\n",
      "ت 1578\n",
      "َ 1614\n",
      "ر 1585\n",
      "ّ 1617\n",
      "َ 1614\n",
      "--------------------\n",
      "يّيلعبُُُ\n"
     ]
    }
   ],
   "source": [
    "text = 'تَغْتَرَّ'\n",
    "print(len(text))\n",
    "\n",
    "# shadda and other diacritics are single characters\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ord(text[i]))\n",
    "    \n",
    "print('--------------------')\n",
    "\n",
    "print(\"يّيلعبُُُ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:10:28.191709500Z",
     "start_time": "2023-12-30T18:10:16.188587800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1', 'َ', '؟', '*', '0', '\"', ']', 'ً', 'ٌ', '!', 'ِ', '{', 'ْ', ';', ':', '\\u200f', '[', '.', ' ', '»', '3', '9', 'ٍ', '2', ')', '4', '`', \"'\", '،', '«', '6', 'ُ', '7', '؛', '–', '8', 'ّ', '-', '}', '5', ',', '~', '/', '('}\n"
     ]
    }
   ],
   "source": [
    "# geet all the tashkeel in the text \n",
    "from constants import ARABIC_LETTERS \n",
    "All_NON_Letters = set()\n",
    "for text in read_dataset():\n",
    "    for c in text:\n",
    "        if c not in ARABIC_LETTERS:\n",
    "            All_NON_Letters.add(c)\n",
    "            \n",
    "print(All_NON_Letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:10:29.100745100Z",
     "start_time": "2023-12-30T18:10:28.190709600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ز؟:.+_@#$.زتَغْتَرَّ//،،)()( جداى آيئءؤرلاىةوزظلآج)\n",
      "قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ( 14 / 123 )\n",
      "قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ( / )\n",
      "---------------------\n",
      "1 َ ؟ * 0 \" ] ً ٌ ! ِ { ْ ; : ‏ [ .   » 3 9 ٍ 2 ) 4 ` ' ، « 6 ُ 7 ؛ – 8 ّ - } 5 , ~ / (\n",
      "َ ؟ * \" ] ً ٌ ! ِ { ْ ; : ‏ [ . » ٍ ) ` ' ، « ُ ؛ – ّ - } , ~ / (\n"
     ]
    }
   ],
   "source": [
    "from regex import P\n",
    "from constants.arabic import HARAKAT\n",
    "from constants import PUNCTUATIONS\n",
    "from constants.arabic import ARABIC_NUMBERS\n",
    "\n",
    "import re\n",
    "import re\n",
    "from constants.arabic import ARABIC_NUMBERS\n",
    "from constants import PUNCTUATIONS\n",
    "# Compile regular expressions\n",
    "number_regex = re.compile(r'[0-9]')\n",
    "arabic_number_regex = re.compile(r'[' + \"\".join(ARABIC_NUMBERS) + ']')\n",
    "punctuation_regex = re.compile(r'[' + \"\".join(PUNCTUATIONS) + ']')\n",
    "spaces_regex = re.compile(r'\\s+')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Delete all numbers and punctuation\n",
    "    # Remove numbers\n",
    "    text = number_regex.sub('', text)\n",
    "    text = arabic_number_regex.sub('', text)\n",
    "    # Remove punctuation marks\n",
    "    # text = punctuation_regex.sub('', text)\n",
    "    # Compress all spaces\n",
    "    text = spaces_regex.sub(' ', text)\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = '   0110ز؟:.+_@#$.زتَغْتَرَّ2//،،312)()( جداى آيئءؤرلاىةوزظلآج)'\n",
    "print(clean_text(text))\n",
    "\n",
    "dataset = read_dataset()\n",
    "\n",
    "for t in dataset[:5]:\n",
    "    print(t)\n",
    "    print(clean_text(t))\n",
    "    print('---------------------')\n",
    "    break\n",
    "\n",
    "non_letters = ' '.join(list(All_NON_Letters))\n",
    "print(non_letters)\n",
    "cleaned = clean_text(non_letters)\n",
    "print(cleaned) # only tashkeel  and spaces left -> for now \n",
    "# print(PUNCTUATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:11:31.245736Z",
     "start_time": "2023-12-30T18:10:29.107748600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:21<00:00, 2362.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['بين', 'في', 'شرح', 'الروض', 'أن', 'إجزاء', 'ذبحه', 'في', 'سنة', 'القضاء', 'بعد', 'دخول', 'وقته', 'وقبل', 'الإحرام', 'به', 'هو', 'ما', 'دل', 'عليه', 'كلام', 'أصله', 'تبعا', 'للعراقيين', '،', 'وأن', 'ما', 'وقع', 'في', 'الروض', 'مما', 'يخالف', 'ذلك', 'من', 'تصرفه', 'قال', 'هكذا', 'أفهم', 'ولا', 'تغتر', 'بما', 'يخالفه']\n",
      " بين في شرح الروض أن إجزاء ذبحه في سنة القضاء بعد دخول وقته وقبل الإحرام به هو ما دل عليه كلام أصله تبعا للعراقيين ، وأن ما وقع في الروض مما يخالف ذلك من تصرفه قال هكذا أفهم ولا تغتر بما يخالفه \n",
      "['$', 'َ', 'َّ', 'َ', '$', 'ِ', '$', '$', 'َ', 'ْ', 'ِ', '$', '$', '$', 'َّ', 'ْ', 'ِ', '$', 'َ', 'َّ', '$', '$', 'ْ', 'َ', '$', 'َ', '$', 'َ', 'ْ', 'ِ', 'ِ', '$', 'ِ', '$', '$', 'َ', 'َ', 'ِ', '$', '$', 'ْ', 'َ', 'َ', '$', 'ِ', '$', 'َ', 'ْ', 'َ', '$', 'ُ', 'ُ', '$', 'ِ', '$', 'َ', 'ْ', 'ِ', 'ِ', '$', 'َ', 'َ', 'ْ', 'َ', '$', '$', 'ْ', 'ِ', 'ْ', 'َ', '$', 'ِ', '$', 'ِ', 'ِ', '$', 'ُ', 'َ', '$', 'َ', '$', '$', 'َ', 'َّ', '$', 'َ', 'َ', 'ْ', 'ِ', '$', 'َ', 'َ', '$', 'ُ', '$', 'َ', 'ْ', 'ِ', 'ِ', '$', 'َ', 'َ', 'ً', '$', '$', 'ِ', 'ْ', 'ِ', 'َ', '$', 'ِ', 'ِّ', '$', 'َ', '$', '$', '$', 'َ', 'َ', 'َّ', '$', 'َ', '$', '$', 'َ', 'َ', 'َ', '$', 'ِ', '$', '$', '$', '$', 'َّ', 'ْ', 'ِ', '$', 'ِ', 'َّ', '$', '$', 'ُ', 'َ', '$', 'ِ', 'ُ', '$', 'َ', 'ِ', 'َ', '$', 'ِ', 'ْ', '$', 'َ', 'َ', 'ُّ', 'ِ', 'ِ', '$', 'َ', '$', 'َ', '$', 'َ', 'َ', 'َ', '$', '$', 'َ', 'ْ', 'َ', 'ُ', '$', 'َ', 'َ', '$', '$', 'َ', 'ْ', 'َ', 'َّ', '$', 'ِ', 'َ', '$', '$', 'ُ', 'َ', '$', 'ِ', 'ُ', 'ُ', '$']\n"
     ]
    }
   ],
   "source": [
    "# Create X and Y to the model\n",
    "# X is the input and Y is the output (the target)\n",
    "# X is a list of all the characters in the dataset\n",
    "# Y is a list of all Diacritics in the dataset (the target)\n",
    "# if the character has no diacritic, the diacritic is set to be $ (empty diacritic)\n",
    "from constants.arabic import HARAKAT,SHADDA,ARABIC_LETTERS\n",
    "from train_collections import harakat2id\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "train = read_dataset()\n",
    "\n",
    "\n",
    "def xy_dataset(dataset):\n",
    "    \"\"\"\n",
    "    dataset: pandas series of the dataset (each example is a string)\n",
    "    return: X, Y as lists\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    X_words = []\n",
    "    Y = []\n",
    "    \n",
    "    for line in tqdm(dataset):\n",
    "        cleaned_line = clean_text(line)\n",
    "        delimiters = [\",\", \"|\", \";\",\"؛\",\".\",\":\", \"(\" , \")\",\"<\",\">\",\"[\",\"]\",\"{\",\"}\"]\n",
    "        \n",
    "        splited_lines = re.split('['+''.join(map(re.escape, delimiters))+']', cleaned_line)\n",
    "        for cleaned_line in splited_lines:\n",
    "            if len(cleaned_line) < 4:\n",
    "                continue\n",
    "            \n",
    "            line_x = []\n",
    "            line_y = []\n",
    "            i = 0\n",
    "            while i < len(cleaned_line):\n",
    "                c = cleaned_line[i]\n",
    "                line_x.append(c)\n",
    "                \n",
    "                # if this is the last character in the line or the next character is not a tashkeel \n",
    "                # then the tashkeel is empty\n",
    "                if i == len(cleaned_line) - 1 or cleaned_line[i+1] not in HARAKAT:\n",
    "                    line_y.append('$')\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                i += 1\n",
    "                tashkeel = cleaned_line[i]\n",
    "                # if this is a shadda, we need to add the next character to the tashkeel\n",
    "                # as shadda  ّ dont come alone\n",
    "                if tashkeel == SHADDA and (i<len(cleaned_line) -1 and cleaned_line[i+1] in HARAKAT):\n",
    "                    i += 1\n",
    "                    tashkeel += cleaned_line[i]\n",
    "                \n",
    "                line_y.append(tashkeel)\n",
    "                \n",
    "                i+=1\n",
    "            # add words to X_words\n",
    "            X_words.append(''.join(line_x).split())\n",
    "            X.append(line_x)\n",
    "            Y.append(line_y)\n",
    "\n",
    "    return X_words, X, Y\n",
    "train_set = read_dataset()\n",
    "X_words , X, Y = xy_dataset(train_set)\n",
    "\n",
    "# make sure they are the same length (each x has a y of the same length)\n",
    "for x, y in zip(X, Y):\n",
    "    assert len(x) == len(y)\n",
    "\n",
    "print(X_words[544])\n",
    "print(''.join(X[544]))\n",
    "print(Y[544])\n",
    "\n",
    "def convert_to_gold_standard_format(X, Y,name='train'):\n",
    "    \"\"\"\n",
    "    X is a list of lists of characters\n",
    "    Y is a list of lists of diacritics\n",
    "    return is a csv file in the gold standard format \n",
    "    ID,label\n",
    "    \"\"\"\n",
    "    pfile = pd.DataFrame(columns=['ID','letter','label'])\n",
    "    pairs = []\n",
    "    for sent, tags in zip(X, Y):\n",
    "        for c, t in zip(sent, tags):\n",
    "            if c in ARABIC_LETTERS:\n",
    "                if t == '$':\n",
    "                    t = ''\n",
    "                pairs.append([c,harakat2id[t]])\n",
    "                \n",
    "    pfile['ID'] = [i for i in range(len(pairs))]  \n",
    "    pfile['letter'] = [pair[0] for pair in pairs]\n",
    "    pfile['label'] = [pair[1] for pair in pairs]\n",
    "    pfile.to_csv(f\"./clean_out/{name}_gold\"+'.csv',index=False)\n",
    "        \n",
    "    \n",
    "                \n",
    "            \n",
    "# save outs into 3 files in CLEAN_OUT_FOLDER   \n",
    "\n",
    "def save_dataset(X, Y, X_words, x_file='X.csv', y_file='Y.csv', x_words_file='X_words.txt'):\n",
    "    \"\"\"\n",
    "    save X, Y, X_words into 3 files in CLEAN_OUT_FOLDER\n",
    "    \"\"\"\n",
    "    # make sure the folder exists\n",
    "    if not os.path.exists(CLEAN_OUT_FOLDER):\n",
    "        os.makedirs(CLEAN_OUT_FOLDER)\n",
    "    # save X\n",
    "    with open(os.path.join(CLEAN_OUT_FOLDER, x_file), 'w', encoding=\"utf8\") as f:\n",
    "        for line in X:\n",
    "            f.write('s'.join(line) + '\\n')\n",
    "    \n",
    "    # save Y\n",
    "    with open(os.path.join(CLEAN_OUT_FOLDER, y_file), 'w', encoding=\"utf8\") as f:\n",
    "        for line in Y:\n",
    "            f.write('s'.join(line) + '\\n')\n",
    "    \n",
    "    # save X_words\n",
    "    with open(os.path.join(CLEAN_OUT_FOLDER, x_words_file), 'w', encoding=\"utf8\") as f:\n",
    "        for line in X_words:\n",
    "            f.write(' '.join(line) + '\\n')\n",
    "    \n",
    "save_dataset(X, Y, X_words)\n",
    "\n",
    "convert_to_gold_standard_format(X, Y)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:11:31.265315400Z",
     "start_time": "2023-12-30T18:11:31.256253400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n",
      "194\n",
      "  $\n",
      "ب َ\n",
      "ي َّ\n",
      "ن َ\n",
      "  $\n",
      "ف ِ\n",
      "ي $\n",
      "  $\n",
      "ش َ\n",
      "ر ْ\n",
      "ح ِ\n",
      "  $\n",
      "ا $\n",
      "ل $\n",
      "ر َّ\n",
      "و ْ\n",
      "ض ِ\n",
      "  $\n",
      "أ َ\n",
      "ن َّ\n",
      "  $\n",
      "إ $\n",
      "ج ْ\n",
      "ز َ\n",
      "ا $\n",
      "ء َ\n",
      "  $\n",
      "ذ َ\n",
      "ب ْ\n",
      "ح ِ\n",
      "ه ِ\n",
      "  $\n",
      "ف ِ\n",
      "ي $\n",
      "  $\n",
      "س َ\n",
      "ن َ\n",
      "ة ِ\n",
      "  $\n",
      "ا $\n",
      "ل ْ\n",
      "ق َ\n",
      "ض َ\n",
      "ا $\n",
      "ء ِ\n",
      "  $\n",
      "ب َ\n",
      "ع ْ\n",
      "د َ\n",
      "  $\n",
      "د ُ\n",
      "خ ُ\n",
      "و $\n",
      "ل ِ\n",
      "  $\n",
      "و َ\n",
      "ق ْ\n",
      "ت ِ\n",
      "ه ِ\n",
      "  $\n",
      "و َ\n",
      "ق َ\n",
      "ب ْ\n",
      "ل َ\n",
      "  $\n",
      "ا $\n",
      "ل ْ\n",
      "إ ِ\n",
      "ح ْ\n",
      "ر َ\n",
      "ا $\n",
      "م ِ\n",
      "  $\n",
      "ب ِ\n",
      "ه ِ\n",
      "  $\n",
      "ه ُ\n",
      "و َ\n",
      "  $\n",
      "م َ\n",
      "ا $\n",
      "  $\n",
      "د َ\n",
      "ل َّ\n",
      "  $\n",
      "ع َ\n",
      "ل َ\n",
      "ي ْ\n",
      "ه ِ\n",
      "  $\n",
      "ك َ\n",
      "ل َ\n",
      "ا $\n",
      "م ُ\n",
      "  $\n",
      "أ َ\n",
      "ص ْ\n",
      "ل ِ\n",
      "ه ِ\n",
      "  $\n",
      "ت َ\n",
      "ب َ\n",
      "ع ً\n",
      "ا $\n",
      "  $\n",
      "ل ِ\n",
      "ل ْ\n",
      "ع ِ\n",
      "ر َ\n",
      "ا $\n",
      "ق ِ\n",
      "ي ِّ\n",
      "ي $\n",
      "ن َ\n",
      "  $\n",
      "، $\n",
      "  $\n",
      "و َ\n",
      "أ َ\n",
      "ن َّ\n",
      "  $\n",
      "م َ\n",
      "ا $\n",
      "  $\n",
      "و َ\n",
      "ق َ\n",
      "ع َ\n",
      "  $\n",
      "ف ِ\n",
      "ي $\n",
      "  $\n",
      "ا $\n",
      "ل $\n",
      "ر َّ\n",
      "و ْ\n",
      "ض ِ\n",
      "  $\n",
      "م ِ\n",
      "م َّ\n",
      "ا $\n",
      "  $\n",
      "ي ُ\n",
      "خ َ\n",
      "ا $\n",
      "ل ِ\n",
      "ف ُ\n",
      "  $\n",
      "ذ َ\n",
      "ل ِ\n",
      "ك َ\n",
      "  $\n",
      "م ِ\n",
      "ن ْ\n",
      "  $\n",
      "ت َ\n",
      "ص َ\n",
      "ر ُّ\n",
      "ف ِ\n",
      "ه ِ\n",
      "  $\n",
      "ق َ\n",
      "ا $\n",
      "ل َ\n",
      "  $\n",
      "ه َ\n",
      "ك َ\n",
      "ذ َ\n",
      "ا $\n",
      "  $\n",
      "أ َ\n",
      "ف ْ\n",
      "ه َ\n",
      "م ُ\n",
      "  $\n",
      "و َ\n",
      "ل َ\n",
      "ا $\n",
      "  $\n",
      "ت َ\n",
      "غ ْ\n",
      "ت َ\n",
      "ر َّ\n",
      "  $\n",
      "ب ِ\n",
      "م َ\n",
      "ا $\n",
      "  $\n",
      "ي ُ\n",
      "خ َ\n",
      "ا $\n",
      "ل ِ\n",
      "ف ُ\n",
      "ه ُ\n",
      "  $\n"
     ]
    }
   ],
   "source": [
    "# print them side by side \n",
    "print(len(X[544]))\n",
    "print(len(Y[544]))\n",
    "for x, y in zip(X[544], Y[544]):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:11:34.707957Z",
     "start_time": "2023-12-30T18:11:31.267321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:  (2500,)\n",
      "first 5 examples: \n",
      "0       ( 27 ) قَوْلُهُ : وَلَا تُكْرَهُ ضِيَافَتُهُ .\n",
      "1    ( الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ ...\n",
      "2    ( قَوْلُهُ : وَهُوَ ) أَيْ : الْبَيْعُ بِالْمَ...\n",
      "3    وَالْعَفْوُ قَبْلَ الْإِمَامِ ، أَوْ بَعْدَهُ ...\n",
      "4    ( قَوْلُهُ : وَرِبْحُهُ ) أَيْ الْقِرَاضِ وَقَ...\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:00<00:00, 4860.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# val dataset too \n",
    "# val_dataset_f = os.path.join(DATASET_FOLDER, 'val.txt')\n",
    "# read dataset as a text file and each line as a training example\n",
    "val_dataset = read_dataset('val.txt', verbose=True)\n",
    "X_words , X, Y = xy_dataset(val_dataset)\n",
    "convert_to_gold_standard_format(X, Y,name='val')\n",
    "save_dataset(X, Y, X_words, x_file='X_val.csv', y_file='Y_val.csv', x_words_file='X_words_val.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:11:37.786637100Z",
     "start_time": "2023-12-30T18:11:34.710507400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:  (2500,)\n",
      "first 5 examples: \n",
      "0    ( قَوْلُهُ : وَلَوْ ادَّعَى وَلَدَ أَمَةٍ مُشْ...\n",
      "1    قَوْلُهُ : ( وَبَحَثَ الرَّافِعِيُّ صِحَّتَهَا...\n",
      "2    وَالْهَاوَنُ مِثَالٌ ، فَمِثْلُهُ كُلُّ مَا يَ...\n",
      "3    وَيَرُدُّ عَلَيْهِ أَنَّ فَاقِدَ الطَّهُورَيْن...\n",
      "4    كُهَيْلٍ قَالَ سَمِعْتُ أَبِي يُحَدِّثُ عَنْ ح...\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:00<00:00, 4977.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# test dataset too\n",
    "test_dataset = read_dataset('test.txt', verbose=True)\n",
    "X_words , X, Y = xy_dataset(test_dataset)\n",
    "convert_to_gold_standard_format(X, Y,name='test')\n",
    "save_dataset(X, Y, X_words, x_file='X_test.csv', y_file='Y_test.csv', x_words_file='X_words_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:11:38.317419800Z",
     "start_time": "2023-12-30T18:11:37.791637600Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_all_text(file_names: list[str]):\n",
    "    \"\"\"\n",
    "    takes a list of file names and merge them into one text file\n",
    "    \"\"\"\n",
    "    with open(f'{CLEAN_OUT_FOLDER}/merged.txt', 'w', encoding=\"utf8\") as outfile:\n",
    "        for fname in file_names:\n",
    "            with open(fname, encoding=\"utf8\") as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "                    \n",
    "cln_train = os.path.join(CLEAN_OUT_FOLDER, 'X_words.txt')\n",
    "cln_val = os.path.join(CLEAN_OUT_FOLDER, 'X_words_val.txt')    \n",
    "merge_all_text([cln_train, cln_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T22:06:39.925632Z",
     "start_time": "2023-12-30T22:06:39.911455300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'رحال'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.arlstem import ARLSTem\n",
    "\n",
    "stemmer = ARLSTem()\n",
    "stemmer.stem('رحالة')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
