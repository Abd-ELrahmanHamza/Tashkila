{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:49:19.041622900Z",
     "start_time": "2023-12-06T18:49:17.859739700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "DATASET_FOLDER = 'dataset'\n",
    "CLEAN_OUT_FOLDER = 'clean_out'\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:49:28.581079500Z",
     "start_time": "2023-12-06T18:49:28.568660400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:  (50000,)\n",
      "first 5 examples: \n",
      "0    قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَ...\n",
      "1    ابْنُ عَرَفَةَ : قَوْلُهُ : بِلَفْظٍ يَقْتَضِي...\n",
      "2    ( قَوْلُهُ لِعَدَمِ مَا تَتَعَلَّقُ إلَخْ ) أَ...\n",
      "3                       وَحَيَوَانٌ غَيْرُ مَوْجُودٍ .\n",
      "4    فَائِدَةٌ : قَالَ بَعْضُهُمْ : يُؤْخَذُ مِنْ ش...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# read dataset train \n",
    "def read_dataset(file_name = 'train.txt' ,verbose=False):\n",
    "    \"\"\"\n",
    "    takes a file name and returns a pandas dataframe of the dataset \n",
    "    the returnes is a pandas series of the dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = os.path.join(DATASET_FOLDER, file_name)\n",
    "    # read dataset as a text file and each line as a training example\n",
    "    dataset = pd.read_csv(dataset, sep='\\t', header=None).squeeze('columns')\n",
    "    \n",
    "    if verbose:\n",
    "        print('dataset shape: ', dataset.shape)\n",
    "        print('first 5 examples: ')\n",
    "        print(dataset[:5])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "read_dataset(verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:10:16.181506300Z",
     "start_time": "2023-12-30T18:10:16.169512Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "ت 1578\n",
      "َ 1614\n",
      "غ 1594\n",
      "ْ 1618\n",
      "ت 1578\n",
      "َ 1614\n",
      "ر 1585\n",
      "ّ 1617\n",
      "َ 1614\n",
      "--------------------\n",
      "يّيلعبُُُ\n"
     ]
    }
   ],
   "source": [
    "text = 'تَغْتَرَّ'\n",
    "print(len(text))\n",
    "\n",
    "# shadda and other diacritics are single characters\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ord(text[i]))\n",
    "    \n",
    "print('--------------------')\n",
    "\n",
    "print(\"يّيلعبُُُ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'–', '*', '3', '(', '4', 'ٍ', '7', '}', 'ْ', \"'\", '8', '\\u200f', ']', '9', '[', '0', '؛', '!', 'ً', '.', '`', ')', '-', ' ', '6', '5', ';', 'َ', '~', '1', ':', 'ِ', '/', '2', '؟', '\"', 'ّ', '»', ',', '،', '{', 'ُ', 'ٌ', '«'}\n"
     ]
    }
   ],
   "source": [
    "# geet all the tashkeel in the text \n",
    "from constants import ARABIC_LETTERS \n",
    "All_NON_Letters = set()\n",
    "for text in read_dataset():\n",
    "    for c in text:\n",
    "        if c not in ARABIC_LETTERS:\n",
    "            All_NON_Letters.add(c)\n",
    "            \n",
    "print(All_NON_Letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ز؟:.+_@#$.زتَغْتَرَّ//،،)()( جداى آيئءؤرلاىةوزظلآج)\n",
      "قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ( 14 / 123 )\n",
      "قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ( / )\n",
      "---------------------\n",
      "– * 3 ( 4 ٍ 7 } ْ ' 8 ‏ ] 9 [ 0 ؛ ! ً . ` ) -   6 5 ; َ ~ 1 : ِ / 2 ؟ \" ّ » , ، { ُ ٌ «\n",
      "– * ( ٍ } ْ ' ‏ ] [ ؛ ! ً . ` ) - ; َ ~ : ِ / ؟ \" ّ » , ، { ُ ٌ «\n"
     ]
    }
   ],
   "source": [
    "from constants.arabic import HARAKAT\n",
    "from constants import PUNCTUATIONS\n",
    "from constants.arabic import ARABIC_NUMBERS\n",
    "\n",
    "import re\n",
    "from constants.arabic import ARABIC_NUMBERS\n",
    "from constants import PUNCTUATIONS\n",
    "# Compile regular expressions\n",
    "number_regex = re.compile(r'[0-9]')\n",
    "arabic_number_regex = re.compile(r'[' + \"\".join(ARABIC_NUMBERS) + ']')\n",
    "punctuation_regex = re.compile(r'[' + \"\".join(PUNCTUATIONS) + ']')\n",
    "spaces_regex = re.compile(r'\\s+')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Delete all numbers and punctuation\n",
    "    # Remove numbers\n",
    "    text = number_regex.sub('', text)\n",
    "    text = arabic_number_regex.sub('', text)\n",
    "    # Remove punctuation marks\n",
    "    # text = punctuation_regex.sub('', text)\n",
    "    # Compress all spaces\n",
    "    text = spaces_regex.sub(' ', text)\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = '0110ز؟:.+_@#$.زتَغْتَرَّ2//،،312)()( جداى آيئءؤرلاىةوزظلآج)'\n",
    "print(clean_text(text))\n",
    "\n",
    "dataset = read_dataset()\n",
    "\n",
    "for t in dataset[:5]:\n",
    "    print(t)\n",
    "    print(clean_text(t))\n",
    "    print('---------------------')\n",
    "    break\n",
    "\n",
    "non_letters = ' '.join(list(All_NON_Letters))\n",
    "print(non_letters)\n",
    "cleaned = clean_text(non_letters)\n",
    "print(cleaned) # only tashkeel  and spaces left -> for now \n",
    "# print(PUNCTUATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:11<00:00, 4339.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create X and Y to the model\n",
    "# X is the input and Y is the output (the target)\n",
    "# X is a list of all the characters in the dataset\n",
    "# Y is a list of all Diacritics in the dataset (the target)\n",
    "# if the character has no diacritic, the diacritic is set to be $ (empty diacritic)\n",
    "from constants.arabic import HARAKAT,SHADDA,ARABIC_LETTERS\n",
    "from train_collections import harakat2id\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "train = read_dataset()\n",
    "\n",
    "\n",
    "def xy_dataset(dataset,val_mode=False):\n",
    "    \"\"\"\n",
    "    dataset: pandas series of the dataset (each example is a string)\n",
    "    return: X, Y as lists\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    X_words = []\n",
    "    Y = []\n",
    "    \n",
    "    for line in tqdm(dataset):\n",
    "        cleaned_line = clean_text(line)\n",
    "        delimiters = [\",\", \"|\", \";\",\"؛\",\".\",\":\", \"(\" , \")\",\"<\",\">\",\"[\",\"]\",\"{\",\"}\"]\n",
    "        splited_lines = re.split('['+''.join(map(re.escape, delimiters))+']', cleaned_line)\n",
    "        for cleaned_line in splited_lines:\n",
    "            \n",
    "            if len(cleaned_line) < 1 or  (len(cleaned_line) < 4 and not val_mode):\n",
    "                continue\n",
    "            \n",
    "            line_x = []\n",
    "            line_y = []\n",
    "            i = 0\n",
    "            while i < len(cleaned_line):\n",
    "                c = cleaned_line[i]\n",
    "                line_x.append(c)\n",
    "                \n",
    "                # if this is the last character in the line or the next character is not a tashkeel \n",
    "                # then the tashkeel is empty\n",
    "                if i == len(cleaned_line) - 1 or cleaned_line[i+1] not in HARAKAT:\n",
    "                    line_y.append('$')\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                i += 1\n",
    "                tashkeel = cleaned_line[i]\n",
    "                # if this is a shadda, we need to add the next character to the tashkeel\n",
    "                # as shadda  ّ dont come alone\n",
    "                if tashkeel == SHADDA and (i<len(cleaned_line) -1 and cleaned_line[i+1] in HARAKAT):\n",
    "                    i += 1\n",
    "                    tashkeel += cleaned_line[i]\n",
    "                \n",
    "                line_y.append(tashkeel)\n",
    "                \n",
    "                i+=1\n",
    "            # add words to X_words\n",
    "            X_words.append(''.join(line_x).split())\n",
    "            X.append(line_x)\n",
    "            Y.append(line_y)\n",
    "\n",
    "    return X_words, X, Y\n",
    "train_set = read_dataset()\n",
    "X_words , X, Y = xy_dataset(train_set)\n",
    "\n",
    "\n",
    "\n",
    "# make sure they are the same length (each x has a y of the same length)\n",
    "for x, y in zip(X, Y):\n",
    "    assert len(x) == len(y)\n",
    "\n",
    "\n",
    "def convert_to_gold_standard_format(X, Y,name='train'):\n",
    "    \"\"\"\n",
    "    X is a list of lists of characters\n",
    "    Y is a list of lists of diacritics\n",
    "    return is a csv file in the gold standard format \n",
    "    ID,label\n",
    "    \"\"\"\n",
    "    pfile = pd.DataFrame(columns=['ID','letter','label'])\n",
    "    pairs = []\n",
    "    for sent, tags in zip(X, Y):\n",
    "        for c, t in zip(sent, tags):\n",
    "            if c in ARABIC_LETTERS:\n",
    "                if t == '$':\n",
    "                    t = ''\n",
    "                pairs.append([c,harakat2id[t]])\n",
    "                \n",
    "    pfile['ID'] = [i for i in range(len(pairs))]  \n",
    "    pfile['letter'] = [pair[0] for pair in pairs]\n",
    "    pfile['label'] = [pair[1] for pair in pairs]\n",
    "    pfile.to_csv(f\"./clean_out/{name}_gold\"+'.csv',index=False)\n",
    "        \n",
    "    \n",
    "                \n",
    "            \n",
    "# save outs into 3 files in CLEAN_OUT_FOLDER   \n",
    "\n",
    "def save_dataset(X, Y, X_words,x_file='X.csv', y_file='Y.csv', x_words_file='X_words.txt'):\n",
    "    \"\"\"\n",
    "    save X, Y, X_words into 3 files in CLEAN_OUT_FOLDER\n",
    "    \"\"\"\n",
    "    # make sure the folder exists\n",
    "    if not os.path.exists(CLEAN_OUT_FOLDER):\n",
    "        os.makedirs(CLEAN_OUT_FOLDER)\n",
    "    # save X\n",
    "    with open(os.path.join(CLEAN_OUT_FOLDER, x_file), 'w') as f:\n",
    "        for line in X:\n",
    "            f.write('s'.join(line) + '\\n')\n",
    "    \n",
    "    # save Y\n",
    "    with open(os.path.join(CLEAN_OUT_FOLDER, y_file), 'w') as f:\n",
    "        for line in Y:\n",
    "            f.write('s'.join(line) + '\\n')\n",
    "    \n",
    "    # save X_words\n",
    "    with open(os.path.join(CLEAN_OUT_FOLDER, x_words_file), 'w') as f:\n",
    "        for line in X_words:\n",
    "            f.write(' '.join(line) + '\\n')\n",
    "\n",
    "            \n",
    "    \n",
    "save_dataset(X, Y, X_words)\n",
    "\n",
    "convert_to_gold_standard_format(X, Y)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:  (2500,)\n",
      "first 5 examples: \n",
      "0       ( 27 ) قَوْلُهُ : وَلَا تُكْرَهُ ضِيَافَتُهُ .\n",
      "1    ( الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ ...\n",
      "2    ( قَوْلُهُ : وَهُوَ ) أَيْ : الْبَيْعُ بِالْمَ...\n",
      "3    وَالْعَفْوُ قَبْلَ الْإِمَامِ ، أَوْ بَعْدَهُ ...\n",
      "4    ( قَوْلُهُ : وَرِبْحُهُ ) أَيْ الْقِرَاضِ وَقَ...\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:00<00:00, 5917.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# val dataset too \n",
    "# val_dataset_f = os.path.join(DATASET_FOLDER, 'val.txt')\n",
    "# read dataset as a text file and each line as a training example\n",
    "val_dataset = read_dataset('val.txt', verbose=True)\n",
    "X_words , X, Y = xy_dataset(val_dataset)\n",
    "convert_to_gold_standard_format(X, Y,name='val')\n",
    "save_dataset(X, Y, X_words, x_file='X_val.csv', y_file='Y_val.csv', x_words_file='X_words_val.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:  (1,)\n",
      "first 5 examples: \n",
      "0    صعد أدهم وحمزة الشجرة حتى يقطعا الطريق على الأ...\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# val dataset too \n",
    "# val_dataset_f = os.path.join(DATASET_FOLDER, 'val.txt')\n",
    "# read dataset as a text file and each line as a training example\n",
    "val_dataset = read_dataset('demo.txt', verbose=True)\n",
    "X_words , X, Y = xy_dataset(val_dataset, val_mode=True)\n",
    "convert_to_gold_standard_format(X, Y,name='val')\n",
    "save_dataset(X, Y, X_words, x_file='X_demo.csv', y_file='Y_demo.csv', x_words_file='X_words_demo.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_text(file_names: list[str]):\n",
    "    \"\"\"\n",
    "    takes a list of file names and merge them into one text file\n",
    "    \"\"\"\n",
    "    with open(f'{CLEAN_OUT_FOLDER}/merged.txt', 'w') as outfile:\n",
    "        for fname in file_names:\n",
    "            with open(fname) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "                    \n",
    "cln_train = os.path.join(CLEAN_OUT_FOLDER, 'X_words.txt')\n",
    "cln_val = os.path.join(CLEAN_OUT_FOLDER, 'X_words_val.txt')    \n",
    "merge_all_text([cln_train, cln_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = read_dataset()\n",
    "val = read_dataset('val.txt')\n",
    "with open(f'{CLEAN_OUT_FOLDER}/merged_unsplited.txt', 'w',encoding=\"utf8\") as infile:\n",
    "    trainfile = open(f'{CLEAN_OUT_FOLDER}/train_unsplited.txt','w', encoding=\"utf8\")\n",
    "    valfile = open(f'{CLEAN_OUT_FOLDER}/val_unsplited.txt','w', encoding=\"utf8\")\n",
    "    # keep only arabic letters and spaces\n",
    "    for t in train:\n",
    "        ll = ''.join([c for c in t if c in ARABIC_LETTERS or c == ' '])\n",
    "        ll = re.sub(r'\\s+', ' ', ll)+ '\\n'\n",
    "        trainfile.write(ll)\n",
    "        infile.write(ll)\n",
    "        \n",
    "    for t in val:\n",
    "        ll = ''.join([c for c in t if c in ARABIC_LETTERS or c == ' ']) \n",
    "        ll = re.sub(r'\\s+', ' ', ll)+ '\\n'\n",
    "        valfile.write(ll)\n",
    "        infile.write(ll)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'مدرس'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.arlstem import ARLSTem\n",
    "\n",
    "stemmer = ARLSTem()\n",
    "stemmer.stem('المدرسة')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
