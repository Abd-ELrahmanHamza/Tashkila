{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:49:19.041622900Z",
     "start_time": "2023-12-06T18:49:17.859739700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "DATASET_FOLDER = 'dataset'\n",
    "CLEAN_OUT_FOLDER = 'clean_out'\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:49:28.581079500Z",
     "start_time": "2023-12-06T18:49:28.568660400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:  (50000,)\n",
      "first 5 examples: \n",
      "0    قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَ...\n",
      "1    ابْنُ عَرَفَةَ : قَوْلُهُ : بِلَفْظٍ يَقْتَضِي...\n",
      "2    ( قَوْلُهُ لِعَدَمِ مَا تَتَعَلَّقُ إلَخْ ) أَ...\n",
      "3                       وَحَيَوَانٌ غَيْرُ مَوْجُودٍ .\n",
      "4    فَائِدَةٌ : قَالَ بَعْضُهُمْ : يُؤْخَذُ مِنْ ش...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# read dataset train \n",
    "def read_dataset(file_name = 'train.txt' ,verbose=False):\n",
    "    \"\"\"\n",
    "    takes a file name and returns a pandas dataframe of the dataset \n",
    "    the returnes is a pandas series of the dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = os.path.join(DATASET_FOLDER, file_name)\n",
    "    # read dataset as a text file and each line as a training example\n",
    "    dataset = pd.read_csv(dataset, sep='\\t', header=None).squeeze('columns')\n",
    "    \n",
    "    if verbose:\n",
    "        print('dataset shape: ', dataset.shape)\n",
    "        print('first 5 examples: ')\n",
    "        print(dataset[:5])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "read_dataset(verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "ت 1578\n",
      "َ 1614\n",
      "غ 1594\n",
      "ْ 1618\n",
      "ت 1578\n",
      "َ 1614\n",
      "ر 1585\n",
      "ّ 1617\n",
      "َ 1614\n",
      "--------------------\n",
      "يّيلعبُُُ\n"
     ]
    }
   ],
   "source": [
    "text = 'تَغْتَرَّ'\n",
    "print(len(text))\n",
    "\n",
    "# shadda and other diacritics are single characters\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ord(text[i]))\n",
    "    \n",
    "print('--------------------')\n",
    "\n",
    "print(\"يّيلعبُُُ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{':', '8', '،', '»', '7', '~', '.', ']', 'ً', '!', '\\u200f', 'ٍ', '؟', '[', ')', '5', '4', '3', '9', 'ُ', '*', '«', '(', '\"', '–', '`', '0', '1', ',', '{', '}', 'ٌ', 'ْ', '2', ';', \"'\", '/', '-', '؛', 'ِ', '6', ' ', 'ّ', 'َ'}\n"
     ]
    }
   ],
   "source": [
    "# geet all the tashkeel in the text \n",
    "from constants import ARABIC_LETTERS \n",
    "All_NON_Letters = set()\n",
    "for text in read_dataset():\n",
    "    for c in text:\n",
    "        if c not in ARABIC_LETTERS:\n",
    "            All_NON_Letters.add(c)\n",
    "            \n",
    "print(All_NON_Letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ز؟:.+_@#$.زتَغْتَرَّ//،،)()( جداى آيئءؤرلاىةوزظلآج)\n",
      "قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ( 14 / 123 )\n",
      "قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ( / )\n",
      "---------------------\n",
      ": 8 ، » 7 ~ . ] ً ! ‏ ٍ ؟ [ ) 5 4 3 9 ُ * « ( \" – ` 0 1 , { } ٌ ْ 2 ; ' / - ؛ ِ 6   ّ َ\n",
      ": ، » ~ . ] ً ! ‏ ٍ ؟ [ ) ُ * « ( \" – ` , { } ٌ ْ ; ' / - ؛ ِ ّ َ\n"
     ]
    }
   ],
   "source": [
    "from regex import P\n",
    "from constants.arabic import HARAKAT\n",
    "from constants import PUNCTUATIONS\n",
    "from constants.arabic import ARABIC_NUMBERS\n",
    "\n",
    "import re\n",
    "import re\n",
    "from constants.arabic import ARABIC_NUMBERS\n",
    "from constants import PUNCTUATIONS\n",
    "# Compile regular expressions\n",
    "number_regex = re.compile(r'[0-9]')\n",
    "arabic_number_regex = re.compile(r'[' + \"\".join(ARABIC_NUMBERS) + ']')\n",
    "punctuation_regex = re.compile(r'[' + \"\".join(PUNCTUATIONS) + ']')\n",
    "spaces_regex = re.compile(r'\\s+')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Delete all numbers and punctuation\n",
    "    # Remove numbers\n",
    "    text = number_regex.sub('', text)\n",
    "    text = arabic_number_regex.sub('', text)\n",
    "    # Remove punctuation marks\n",
    "    # text = punctuation_regex.sub('', text)\n",
    "    # Compress all spaces\n",
    "    text = spaces_regex.sub(' ', text)\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = '   0110ز؟:.+_@#$.زتَغْتَرَّ2//،،312)()( جداى آيئءؤرلاىةوزظلآج)'\n",
    "print(clean_text(text))\n",
    "\n",
    "dataset = read_dataset()\n",
    "\n",
    "for t in dataset[:5]:\n",
    "    print(t)\n",
    "    print(clean_text(t))\n",
    "    print('---------------------')\n",
    "    break\n",
    "\n",
    "non_letters = ' '.join(list(All_NON_Letters))\n",
    "print(non_letters)\n",
    "cleaned = clean_text(non_letters)\n",
    "print(cleaned) # only tashkeel  and spaces left -> for now \n",
    "# print(PUNCTUATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:15<00:00, 3295.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['أي', ':', 'وبعد', 'ارتفاع', 'العقد', 'يملك', 'العامل', 'الربح', 'المشروط', 'له']\n",
      " أي : وبعد ارتفاع العقد يملك العامل الربح المشروط له \n",
      "['$', 'َ', 'ْ', '$', '$', '$', 'َ', 'َ', 'ْ', 'َ', '$', '$', 'ْ', 'ِ', 'َ', '$', 'ِ', '$', '$', 'ْ', 'َ', 'ْ', 'ِ', '$', 'َ', 'ْ', 'ِ', 'ُ', '$', '$', 'ْ', 'َ', '$', 'ِ', 'ُ', '$', '$', '$', 'ِّ', 'ْ', 'َ', '$', '$', 'ْ', 'َ', 'ْ', 'ُ', '$', 'َ', '$', 'َ', 'ُ', '$']\n"
     ]
    }
   ],
   "source": [
    "# Create X and Y to the model\n",
    "# X is the input and Y is the output (the target)\n",
    "# X is a list of all the characters in the dataset\n",
    "# Y is a list of all Diacritics in the dataset (the target)\n",
    "# if the character has no diacritic, the diacritic is set to be $ (empty diacritic)\n",
    "from constants.arabic import HARAKAT,SHADDA\n",
    "from tqdm import tqdm\n",
    "train = read_dataset()\n",
    "\n",
    "\n",
    "def xy_dataset(dataset):\n",
    "    \"\"\"\n",
    "    dataset: pandas series of the dataset (each example is a string)\n",
    "    return: X, Y as lists\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    X_words = []\n",
    "    Y = []\n",
    "    \n",
    "    for line in tqdm(dataset):\n",
    "        cleaned_line = clean_text(line)\n",
    "        delimiters = [\",\", \"|\", \";\",\"؛\",\".\", \"(\" , \")\",\"<\",\">\",\"[\",\"]\",\"{\",\"}\"]\n",
    "        \n",
    "        splited_lines = re.split('|'.join(map(re.escape, delimiters)), cleaned_line)\n",
    "        for cleaned_line in splited_lines:\n",
    "            if len(cleaned_line) < 4:\n",
    "                continue\n",
    "            \n",
    "            line_x = []\n",
    "            line_y = []\n",
    "            i = 0\n",
    "            while i < len(cleaned_line):\n",
    "                c = cleaned_line[i]\n",
    "                line_x.append(c)\n",
    "                \n",
    "                # if this is the last character in the line or the next character is not a tashkeel \n",
    "                # then the tashkeel is empty\n",
    "                if i == len(cleaned_line) - 1 or cleaned_line[i+1] not in HARAKAT:\n",
    "                    line_y.append('$')\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                i += 1\n",
    "                tashkeel = cleaned_line[i]\n",
    "                # if this is a shadda, we need to add the next character to the tashkeel\n",
    "                # as shadda  ّ dont come alone\n",
    "                if tashkeel == SHADDA and( i < len(cleaned_line) -1 and cleaned_line[i+1] in HARAKAT):\n",
    "                    i += 1\n",
    "                    tashkeel += cleaned_line[i]\n",
    "                \n",
    "                line_y.append(tashkeel)\n",
    "                \n",
    "                i+=1\n",
    "            # add words to X_words\n",
    "            X_words.append(''.join(line_x).split())\n",
    "            X.append(line_x)\n",
    "            Y.append(line_y)\n",
    "\n",
    "    return X_words, X, Y\n",
    "train_set = read_dataset()\n",
    "X_words , X, Y = xy_dataset(train_set)\n",
    "\n",
    "# make sure they are the same length (each x has a y of the same length)\n",
    "for x, y in zip(X, Y):\n",
    "    assert len(x) == len(y)\n",
    "\n",
    "print(X_words[544])\n",
    "print(''.join(X[544]))\n",
    "print(Y[544])\n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "# save outs into 3 files in CLEAN_OUT_FOLDER   \n",
    "\n",
    "def save_dataset(X, Y, X_words, x_file='X.csv', y_file='Y.csv', x_words_file='X_words.txt'):\n",
    "    \"\"\"\n",
    "    save X, Y, X_words into 3 files in CLEAN_OUT_FOLDER\n",
    "    \"\"\"\n",
    "    # make sure the folder exists\n",
    "    if not os.path.exists(CLEAN_OUT_FOLDER):\n",
    "        os.makedirs(CLEAN_OUT_FOLDER)\n",
    "    # save X\n",
    "    with open(os.path.join(CLEAN_OUT_FOLDER, x_file), 'w') as f:\n",
    "        for line in X:\n",
    "            f.write('s'.join(line) + '\\n')\n",
    "    \n",
    "    # save Y\n",
    "    with open(os.path.join(CLEAN_OUT_FOLDER, y_file), 'w') as f:\n",
    "        for line in Y:\n",
    "            f.write('s'.join(line) + '\\n')\n",
    "    \n",
    "    # save X_words\n",
    "    with open(os.path.join(CLEAN_OUT_FOLDER, x_words_file), 'w') as f:\n",
    "        for line in X_words:\n",
    "            f.write(' '.join(line) + '\\n')\n",
    "    \n",
    "save_dataset(X, Y, X_words)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "53\n",
      "  $\n",
      "أ َ\n",
      "ي ْ\n",
      "  $\n",
      ": $\n",
      "  $\n",
      "و َ\n",
      "ب َ\n",
      "ع ْ\n",
      "د َ\n",
      "  $\n",
      "ا $\n",
      "ر ْ\n",
      "ت ِ\n",
      "ف َ\n",
      "ا $\n",
      "ع ِ\n",
      "  $\n",
      "ا $\n",
      "ل ْ\n",
      "ع َ\n",
      "ق ْ\n",
      "د ِ\n",
      "  $\n",
      "ي َ\n",
      "م ْ\n",
      "ل ِ\n",
      "ك ُ\n",
      "  $\n",
      "ا $\n",
      "ل ْ\n",
      "ع َ\n",
      "ا $\n",
      "م ِ\n",
      "ل ُ\n",
      "  $\n",
      "ا $\n",
      "ل $\n",
      "ر ِّ\n",
      "ب ْ\n",
      "ح َ\n",
      "  $\n",
      "ا $\n",
      "ل ْ\n",
      "م َ\n",
      "ش ْ\n",
      "ر ُ\n",
      "و $\n",
      "ط َ\n",
      "  $\n",
      "ل َ\n",
      "ه ُ\n",
      "  $\n"
     ]
    }
   ],
   "source": [
    "# print them side by side \n",
    "print(len(X[544]))\n",
    "print(len(Y[544]))\n",
    "for x, y in zip(X[544], Y[544]):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:  (2500,)\n",
      "first 5 examples: \n",
      "0       ( 27 ) قَوْلُهُ : وَلَا تُكْرَهُ ضِيَافَتُهُ .\n",
      "1    ( الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ ...\n",
      "2    ( قَوْلُهُ : وَهُوَ ) أَيْ : الْبَيْعُ بِالْمَ...\n",
      "3    وَالْعَفْوُ قَبْلَ الْإِمَامِ ، أَوْ بَعْدَهُ ...\n",
      "4    ( قَوْلُهُ : وَرِبْحُهُ ) أَيْ الْقِرَاضِ وَقَ...\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 643/2500 [00:00<00:00, 3230.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:00<00:00, 3246.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# val dataset too \n",
    "# val_dataset_f = os.path.join(DATASET_FOLDER, 'val.txt')\n",
    "# read dataset as a text file and each line as a training example\n",
    "val_dataset = read_dataset('val.txt', verbose=True)\n",
    "X_words , X, Y = xy_dataset(val_dataset)\n",
    "save_dataset(X, Y, X_words, x_file='X_val.csv', y_file='Y_val.csv', x_words_file='X_words_val.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_text(file_names: list[str]):\n",
    "    \"\"\"\n",
    "    takes a list of file names and merge them into one text file\n",
    "    \"\"\"\n",
    "    with open(f'{CLEAN_OUT_FOLDER}/merged.txt', 'w') as outfile:\n",
    "        for fname in file_names:\n",
    "            with open(fname) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "                    \n",
    "cln_train = os.path.join(CLEAN_OUT_FOLDER, 'X_words.txt')\n",
    "cln_val = os.path.join(CLEAN_OUT_FOLDER, 'X_words_val.txt')    \n",
    "merge_all_text([cln_train, cln_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'مدرس'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.arlstem import ARLSTem\n",
    "\n",
    "stemmer = ARLSTem()\n",
    "stemmer.stem('المدرسة')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
