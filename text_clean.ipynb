{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:49:19.041622900Z",
     "start_time": "2023-12-06T18:49:17.859739700Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_FOLDER = 'dataset'\n",
    "CLEAN_OUT_FOLDER = 'clean_out'\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:49:28.581079500Z",
     "start_time": "2023-12-06T18:49:28.568660400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:  (50000,)\n",
      "first 5 examples: \n",
      "0    قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَ...\n",
      "1    ابْنُ عَرَفَةَ : قَوْلُهُ : بِلَفْظٍ يَقْتَضِي...\n",
      "2    ( قَوْلُهُ لِعَدَمِ مَا تَتَعَلَّقُ إلَخْ ) أَ...\n",
      "3                       وَحَيَوَانٌ غَيْرُ مَوْجُودٍ .\n",
      "4    فَائِدَةٌ : قَالَ بَعْضُهُمْ : يُؤْخَذُ مِنْ ش...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# read dataset train \n",
    "def read_dataset(file_name = 'train.txt' ,verbose=False):\n",
    "    \"\"\"\n",
    "    takes a file name and returns a pandas dataframe of the dataset \n",
    "    the returnes is a pandas series of the dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = os.path.join(DATASET_FOLDER, file_name)\n",
    "    # read dataset as a text file and each line as a training example\n",
    "    dataset = pd.read_csv(dataset, sep='\\t', header=None).squeeze('columns')\n",
    "    \n",
    "    if verbose:\n",
    "        print('dataset shape: ', dataset.shape)\n",
    "        print('first 5 examples: ')\n",
    "        print(dataset[:5])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "read_dataset(verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "ت 1578\n",
      "َ 1614\n",
      "غ 1594\n",
      "ْ 1618\n",
      "ت 1578\n",
      "َ 1614\n",
      "ر 1585\n",
      "ّ 1617\n",
      "َ 1614\n",
      "--------------------\n",
      "يّيلعبُُُ\n"
     ]
    }
   ],
   "source": [
    "text = 'تَغْتَرَّ'\n",
    "print(len(text))\n",
    "\n",
    "# shadda and other diacritics are single characters\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ord(text[i]))\n",
    "    \n",
    "print('--------------------')\n",
    "\n",
    "print(\"يّيلعبُُُ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0', '؟', '»', '6', '`', '؛', ',', 'ً', '{', '.', ':', '–', 'َ', '-', 'ٌ', '7', 'ّ', ';', '،', '5', '!', 'ٍ', '~', '4', ' ', '1', '*', '\\u200f', '/', 'ُ', '8', '\"', '(', 'ْ', '«', \"'\", ')', '[', '3', ']', '9', '2', '}', 'ِ'}\n"
     ]
    }
   ],
   "source": [
    "# geet all the tashkeel in the text \n",
    "from constants import ARABIC_LETTERS \n",
    "All_NON_Letters = set()\n",
    "for text in read_dataset():\n",
    "    for c in text:\n",
    "        if c not in ARABIC_LETTERS:\n",
    "            All_NON_Letters.add(c)\n",
    "            \n",
    "print(All_NON_Letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ززتَغْتَرَّ جداى آيئءؤرلاىةوزظلآج\n",
      "قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ( 14 / 123 )\n",
      "قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ\n",
      "---------------------\n",
      "0 ؟ » 6 ` ؛ , ً { . : – َ - ٌ 7 ّ ; ، 5 ! ٍ ~ 4   1 * ‏ / ُ 8 \" ( ْ « ' ) [ 3 ] 9 2 } ِ\n",
      "ً َ ٌ ّ ٍ ‏ ُ ْ ِ\n"
     ]
    }
   ],
   "source": [
    "from regex import P\n",
    "from constants.arabic import HARAKAT\n",
    "from constants import PUNCTUATIONS\n",
    "from constants.arabic import ARABIC_NUMBERS\n",
    "\n",
    "import re\n",
    "import re\n",
    "from constants.arabic import ARABIC_NUMBERS\n",
    "from constants import PUNCTUATIONS\n",
    "# Compile regular expressions\n",
    "number_regex = re.compile(r'[0-9]')\n",
    "arabic_number_regex = re.compile(r'[' + \"\".join(ARABIC_NUMBERS) + ']')\n",
    "punctuation_regex = re.compile(r'[' + \"\".join(PUNCTUATIONS) + ']')\n",
    "spaces_regex = re.compile(r'\\s+')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Delete all numbers and punctuation\n",
    "    # Remove numbers\n",
    "    text = number_regex.sub('', text)\n",
    "    text = arabic_number_regex.sub('', text)\n",
    "    # Remove punctuation marks\n",
    "    text = punctuation_regex.sub('', text)\n",
    "    # Compress all spaces\n",
    "    text = spaces_regex.sub(' ', text)\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = '   0110ز؟:.+_@#$.زتَغْتَرَّ2//،،312)()( جداى آيئءؤرلاىةوزظلآج)'\n",
    "print(clean_text(text))\n",
    "\n",
    "dataset = read_dataset()\n",
    "\n",
    "for t in dataset[:5]:\n",
    "    print(t)\n",
    "    print(clean_text(t))\n",
    "    print('---------------------')\n",
    "    break\n",
    "\n",
    "non_letters = ' '.join(list(All_NON_Letters))\n",
    "print(non_letters)\n",
    "cleaned = clean_text(non_letters)\n",
    "print(cleaned) # only tashkeel  and spaces left -> for now \n",
    "# print(PUNCTUATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6394.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي']\n",
      "قوله أو قطع الأول يده إلخ قال الزركشي\n",
      "['َ', 'ْ', 'ُ', 'ُ', '$', 'َ', 'ْ', '$', 'َ', 'َ', 'َ', '$', '$', 'ْ', 'َ', 'َّ', 'ُ', '$', 'َ', 'َ', 'ُ', '$', '$', 'َ', 'ْ', '$', 'َ', '$', 'َ', '$', '$', '$', 'َّ', 'ْ', 'َ', 'ِ', 'ُّ']\n",
      "ق َ\n",
      "و ْ\n",
      "ل ُ\n",
      "ه ُ\n",
      "  $\n",
      "أ َ\n",
      "و ْ\n",
      "  $\n",
      "ق َ\n",
      "ط َ\n",
      "ع َ\n",
      "  $\n",
      "ا $\n",
      "ل ْ\n",
      "أ َ\n",
      "و َّ\n",
      "ل ُ\n",
      "  $\n",
      "ي َ\n",
      "د َ\n",
      "ه ُ\n",
      "  $\n",
      "إ $\n",
      "ل َ\n",
      "خ ْ\n",
      "  $\n",
      "ق َ\n",
      "ا $\n",
      "ل َ\n",
      "  $\n",
      "ا $\n",
      "ل $\n",
      "ز َّ\n",
      "ر ْ\n",
      "ك َ\n",
      "ش ِ\n",
      "ي ُّ\n"
     ]
    }
   ],
   "source": [
    "# Create X and Y to the model\n",
    "# X is the input and Y is the output (the target)\n",
    "# X is a list of all the characters in the dataset\n",
    "# Y is a list of all Diacritics in the dataset (the target)\n",
    "# if the character has no diacritic, the diacritic is set to be $ (empty diacritic)\n",
    "from constants.arabic import HARAKAT,SHADDA\n",
    "from tqdm import tqdm\n",
    "train = read_dataset()\n",
    "\n",
    "\n",
    "def xy_dataset(dataset):\n",
    "    \"\"\"\n",
    "    dataset: pandas series of the dataset (each example is a string)\n",
    "    return: X, Y as lists\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    X_words = []\n",
    "    Y = []\n",
    "    \n",
    "    for line in tqdm(dataset):\n",
    "        cleaned_line = clean_text(line)\n",
    "        line_x = []\n",
    "        line_y = []\n",
    "        i = 0\n",
    "        while i < len(cleaned_line):\n",
    "            c = cleaned_line[i]\n",
    "            line_x.append(c)\n",
    "            \n",
    "            # if this is the last character in the line or the next character is not a tashkeel \n",
    "            # then the tashkeel is empty\n",
    "            if i == len(cleaned_line) - 1 or cleaned_line[i+1] not in HARAKAT:\n",
    "                line_y.append('$')\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            i += 1\n",
    "            tashkeel = cleaned_line[i]\n",
    "            # if this is a shadda, we need to add the next character to the tashkeel\n",
    "            # as shadda  ّ dont come alone\n",
    "            if tashkeel == SHADDA and( i < len(cleaned_line) -1 and cleaned_line[i+1] in HARAKAT):\n",
    "                i += 1\n",
    "                tashkeel += cleaned_line[i]\n",
    "            \n",
    "            line_y.append(tashkeel)\n",
    "            \n",
    "            i+=1\n",
    "        # add words to X_words\n",
    "        X_words.append(''.join(line_x).split())\n",
    "        \n",
    "        X.append(line_x)\n",
    "        Y.append(line_y)\n",
    "\n",
    "    return X_words, X, Y\n",
    "train_set = read_dataset()\n",
    "X_words , X, Y = xy_dataset(train_set)\n",
    "\n",
    "print(X_words[0])\n",
    "print(''.join(X[0]))\n",
    "print(Y[0])\n",
    "\n",
    "# print them side by side \n",
    "for x, y in zip(X[0], Y[0]):\n",
    "    print(x, y)\n",
    "                \n",
    "            \n",
    "# save outs into 3 files in CLEAN_OUT_FOLDER   \n",
    "\n",
    "def save_dataset(X, Y, X_words, x_file='X.csv', y_file='Y.csv', x_words_file='X_words.txt'):\n",
    "    \"\"\"\n",
    "    save X, Y, X_words into 3 files in CLEAN_OUT_FOLDER\n",
    "    \"\"\"\n",
    "    # make sure the folder exists\n",
    "    if not os.path.exists(CLEAN_OUT_FOLDER):\n",
    "        os.makedirs(CLEAN_OUT_FOLDER)\n",
    "    # save X\n",
    "    with open(os.path.join(CLEAN_OUT_FOLDER, x_file), 'w') as f:\n",
    "        for line in X:\n",
    "            f.write(','.join(line) + '\\n')\n",
    "    \n",
    "    # save Y\n",
    "    with open(os.path.join(CLEAN_OUT_FOLDER, y_file), 'w') as f:\n",
    "        for line in Y:\n",
    "            f.write(','.join(line) + '\\n')\n",
    "    \n",
    "    # save X_words\n",
    "    with open(os.path.join(CLEAN_OUT_FOLDER, x_words_file), 'w') as f:\n",
    "        for line in X_words:\n",
    "            f.write(' '.join(line) + '\\n')\n",
    "    \n",
    "save_dataset(X, Y, X_words)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:  (2500,)\n",
      "first 5 examples: \n",
      "0       ( 27 ) قَوْلُهُ : وَلَا تُكْرَهُ ضِيَافَتُهُ .\n",
      "1    ( الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ ...\n",
      "2    ( قَوْلُهُ : وَهُوَ ) أَيْ : الْبَيْعُ بِالْمَ...\n",
      "3    وَالْعَفْوُ قَبْلَ الْإِمَامِ ، أَوْ بَعْدَهُ ...\n",
      "4    ( قَوْلُهُ : وَرِبْحُهُ ) أَيْ الْقِرَاضِ وَقَ...\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:02<00:00, 1109.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# val dataset too \n",
    "# val_dataset_f = os.path.join(DATASET_FOLDER, 'val.txt')\n",
    "# read dataset as a text file and each line as a training example\n",
    "val_dataset = read_dataset('val.txt', verbose=True)\n",
    "X_words , X, Y = xy_dataset(val_dataset)\n",
    "save_dataset(X, Y, X_words, x_file='X_val.csv', y_file='Y_val.csv', x_words_file='X_words_val.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_text(file_names: list[str]):\n",
    "    \"\"\"\n",
    "    takes a list of file names and merge them into one text file\n",
    "    \"\"\"\n",
    "    with open(f'{CLEAN_OUT_FOLDER}/merged.txt', 'w') as outfile:\n",
    "        for fname in file_names:\n",
    "            with open(fname) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "                    \n",
    "cln_train = os.path.join(CLEAN_OUT_FOLDER, 'X_words.txt')\n",
    "cln_val = os.path.join(CLEAN_OUT_FOLDER, 'X_words_val.txt')    \n",
    "merge_all_text([cln_train, cln_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'مدرس'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.arlstem import ARLSTem\n",
    "\n",
    "stemmer = ARLSTem()\n",
    "stemmer.stem('المدرسة')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
