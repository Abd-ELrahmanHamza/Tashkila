{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from letters_dataset import LettersDataset\n",
    "import torch.nn as nn\n",
    "from train_collections import *\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# autoreload notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "n_epochs = 10\n",
    "n_hidden = 128\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 415\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset = LettersDataset(device=device)\n",
    "loader = data.DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# load val data\n",
    "# da = LettersDataset('clean_out/X_val.csv', 'clean_out/y_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_chars = dataset.get_input_vocab_size()\n",
    "n_harakat = dataset.get_output_vocab_size()\n",
    "n_harakat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }, filename)\n",
    "    \n",
    "    \n",
    "def load_checkpoint(model, optimizer, filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch'] + 1\n",
    "    loss = checkpoint['loss']\n",
    "    return epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.baseline import BaseLineModel\n",
    "\n",
    "class CharModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(n_chars, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=n_hidden,\n",
    "                            num_layers=1, batch_first=True )\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(n_hidden, n_harakat)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass thru embedding layer\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(self.dropout(x))\n",
    "        return x\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(n_chars, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=n_hidden, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(2*n_hidden, n_harakat)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass thru embedding layer\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(self.dropout(x))\n",
    "        return x\n",
    "    \n",
    "class DiacritizationModel(nn.Module):\n",
    "    def __init__(self, hidden_size=256,embedding_dim=256,in_vocab=25,out_vocab=25):\n",
    "        super(DiacritizationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=in_vocab, embedding_dim=embedding_dim)\n",
    "        \n",
    "        self.blstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.blstm2 = nn.LSTM(input_size=hidden_size*2, hidden_size=hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_size*2, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(512,out_vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.blstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.blstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = BaseLineModel(n_chars, n_harakat,embedding_dim).to(device)\n",
    "# model = BiLSTM().to(device)\n",
    "# load model\n",
    "# model_d = torch.load('models/base.pth')\n",
    "\n",
    "# model = BaseLineModel(n_chars, n_harakat,embedding_dim,use_batch_norm=True).to(device)\n",
    "# model.load_state_dict(model_d)\n",
    "model = DiacritizationModel(in_vocab=n_chars,out_vocab=n_harakat).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_batches = len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 3150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 0: Loss = 2.7231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [05:40,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 100: Loss = 0.1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [07:16,  3.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_38524\\2213105352.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aatta\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\aatta\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Number of batches:\", num_batches)\n",
    "best_model = None\n",
    "best_loss = np.inf\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for i, (X_batch,y_batch) in tqdm(enumerate(loader)):\n",
    "        y_pred = ''\n",
    "        # y_pred = model(X_batch)['diacritics']\n",
    "        y_pred = model(X_batch)\n",
    "        # we transpose because the loss function expects the second dimension to be the classes\n",
    "        # y_pred is now (batch_size, n_classes, seq_len)\n",
    "        y_pred = y_pred.transpose(1, 2) \n",
    "        # print(y_pred.shape)\n",
    "        # print(y_batch.shape)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch %d, batch %d: Loss = %.4f\" % (epoch, i, loss))\n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    torch.save(model.state_dict(), 'models/base.pth')\n",
    "    \n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (X_batch,y_batch) in loader:\n",
    "            # y_pred = model(X_batch)['diacritics']\n",
    "            y_pred = model(X_batch)\n",
    "            y_pred = y_pred.transpose(1, 2) \n",
    "            loss += loss_fn(y_pred, y_batch)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = model.state_dict()\n",
    "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 1129\n",
      "{'ا': 0, 'ب': 1, 'ت': 2, 'ث': 3, 'ج': 4, 'ح': 5, 'خ': 6, 'د': 7, 'ذ': 8, 'ر': 9, 'ز': 10, 'س': 11, 'ش': 12, 'ص': 13, 'ض': 14, 'ط': 15, 'ظ': 16, 'ع': 17, 'غ': 18, 'ف': 19, 'ق': 20, 'ك': 21, 'ل': 22, 'م': 23, 'ن': 24, 'ه': 25, 'و': 26, 'ي': 27, 'ى': 28, 'ة': 29, 'آ': 30, 'أ': 31, 'إ': 32, 'ء': 33, 'ؤ': 34, 'ئ': 35, ' ': 36, '،': 37, '-': 38, '<pad>': 39, '<unk>': 40}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16416\\3208491554.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[1;31m# print(val_dataset.char_encoder.get_id_by_token(UNK_TOKEN))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pad_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m                 \u001b[0mll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_arabic_letter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_dataset = LettersDataset('clean_out/X_val.csv', 'clean_out/y_val.csv',val_mode=True, device=device)   \n",
    "\n",
    "val_loader = data.DataLoader(val_dataset,  batch_size=batch_size)\n",
    "print(val_dataset.char_encoder.word2idx)\n",
    "# evaluaate accuracy on validation set\n",
    "\n",
    "model.eval()\n",
    "letter_haraka = []\n",
    "with torch.no_grad():\n",
    "    for (X_batch,y_batch) in val_loader:\n",
    "        # y_pred = model(X_batch)['diacritics']\n",
    "        y_pred = model(X_batch)\n",
    "        # we transpose because the loss function expects the second dimension to be the classes\n",
    "        # y_pred is now (batch_size, n_classes, seq_len)\n",
    "        y_pred = y_pred.transpose(1, 2) \n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        # Count only non-padding characters\n",
    "        for x,y in zip(X_batch,predicted):\n",
    "            for xx,yy in zip(x,y):\n",
    "                # we reached the end of the sentence\n",
    "                # print(xx.item())\n",
    "                # print(val_dataset.char_encoder.get_pad_id())\n",
    "                # print(val_dataset.char_encoder.get_id_by_token(UNK_TOKEN))\n",
    "                if xx.item() == val_dataset.char_encoder.get_pad_id():\n",
    "                    break\n",
    "                ll = val_dataset.char_encoder.is_arabic_letter(xx.item())\n",
    "                if ll:\n",
    "                    letter_haraka.append([ll,yy.item()])\n",
    "\n",
    "# save ID,Label pairs in a csv file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(letter_haraka, columns=['letter','label'])\n",
    "df.to_csv('./results/letter_haraka.csv', index=True, index_label='ID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.17%\n"
     ]
    }
   ],
   "source": [
    "gold_val = pd.read_csv('clean_out/val_gold.csv',index_col=0)\n",
    "sys_val = pd.read_csv('results/letter_haraka.csv',index_col=0)\n",
    "# Accuracy per letter\n",
    "# print(gold_val.head())\n",
    "# print(sys_val.head())   \n",
    "# print(gold_val.iloc[0]['label'])\n",
    "\n",
    "correct = 0\n",
    "total = len(gold_val)\n",
    "for i in range(total):\n",
    "    # print(gold_val[i][0], sys_val[i][0])\n",
    "    correct +=( gold_val.iloc[i]['label'] == sys_val.iloc[i]['label'])\n",
    "    \n",
    "print(\"Accuracy: %.2f%%\" % (100.0 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model \n",
    "# torch.save(model, 'models/lstm.pth')\n",
    "# save model state dict\n",
    "torch.save(model.state_dict(), 'models/bilstm.pth')\n",
    "# load model state dict\n",
    "# model = BiLSTM()\n",
    "# model.load_state_dict(torch.load('models/bilstm.pth'))\n",
    "# load model\n",
    "# model = torch.load('models/___.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DER of the network on the validation set: 2 %\n"
     ]
    }
   ],
   "source": [
    "print('DER of the network on the validation set: %d %%' % (100 * (1 - correct / total)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 1174\n",
      "{'ا': 0, 'ب': 1, 'ت': 2, 'ث': 3, 'ج': 4, 'ح': 5, 'خ': 6, 'د': 7, 'ذ': 8, 'ر': 9, 'ز': 10, 'س': 11, 'ش': 12, 'ص': 13, 'ض': 14, 'ط': 15, 'ظ': 16, 'ع': 17, 'غ': 18, 'ف': 19, 'ق': 20, 'ك': 21, 'ل': 22, 'م': 23, 'ن': 24, 'ه': 25, 'و': 26, 'ي': 27, 'ى': 28, 'ة': 29, 'آ': 30, 'أ': 31, 'إ': 32, 'ء': 33, 'ؤ': 34, 'ئ': 35, ' ': 36, '،': 37, '-': 38, '<pad>': 39, '<unk>': 40}\n"
     ]
    }
   ],
   "source": [
    "test_dataset = LettersDataset('clean_out/X_test.csv', 'clean_out/y_test.csv',val_mode=True, device=device)   \n",
    "\n",
    "val_loader = data.DataLoader(test_dataset,  batch_size=batch_size)\n",
    "print(test_dataset.char_encoder.word2idx)\n",
    "# evaluaate accuracy on validation set\n",
    "\n",
    "model.eval()\n",
    "letter_haraka = []\n",
    "with torch.no_grad():\n",
    "    for (X_batch,y_batch) in val_loader:\n",
    "        # y_pred = model(X_batch)['diacritics']\n",
    "        y_pred = model(X_batch)\n",
    "        # we transpose because the loss function expects the second dimension to be the classes\n",
    "        # y_pred is now (batch_size, n_classes, seq_len)\n",
    "        y_pred = y_pred.transpose(1, 2) \n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        # Count only non-padding characters\n",
    "        for x,y in zip(X_batch,predicted):\n",
    "            for xx,yy in zip(x,y):\n",
    "                # we reached the end of the sentence\n",
    "                # print(xx.item())\n",
    "                # print(test_dataset.char_encoder.get_pad_id())\n",
    "                # print(test_dataset.char_encoder.get_id_by_token(UNK_TOKEN))\n",
    "                if xx.item() == test_dataset.char_encoder.get_pad_id():\n",
    "                    break\n",
    "                ll = test_dataset.char_encoder.is_arabic_letter(xx.item())\n",
    "                if ll:\n",
    "                    letter_haraka.append([ll,yy.item()])\n",
    "\n",
    "# save ID,Label pairs in a csv file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(letter_haraka, columns=['letter','label'])\n",
    "df.to_csv('./results/letter_haraka.csv', index=True, index_label='ID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1247\n",
      "0 6\n",
      "1248\n",
      "0 4\n",
      "1256\n",
      "0 2\n",
      "1635\n",
      "2 3\n",
      "6214\n",
      "14 4\n",
      "7307\n",
      "0 2\n",
      "11233\n",
      "4 14\n",
      "11896\n",
      "4 0\n",
      "13275\n",
      "14 2\n",
      "13901\n",
      "4 2\n",
      "14313\n",
      "2 0\n",
      "14314\n",
      "14 0\n",
      "14315\n",
      "0 8\n",
      "14316\n",
      "2 0\n",
      "14659\n",
      "8 6\n",
      "15954\n",
      "14 2\n",
      "15959\n",
      "14 0\n",
      "19253\n",
      "2 0\n",
      "19330\n",
      "0 2\n",
      "19332\n",
      "4 0\n",
      "22111\n",
      "0 8\n",
      "24393\n",
      "0 4\n",
      "25907\n",
      "8 12\n",
      "26646\n",
      "2 10\n",
      "26975\n",
      "0 4\n",
      "27527\n",
      "5 3\n",
      "29481\n",
      "14 4\n",
      "31622\n",
      "0 2\n",
      "33133\n",
      "3 1\n",
      "35369\n",
      "2 0\n",
      "35423\n",
      "0 4\n",
      "37618\n",
      "6 2\n",
      "38085\n",
      "2 4\n",
      "38171\n",
      "14 2\n",
      "39346\n",
      "2 4\n",
      "39396\n",
      "2 0\n",
      "39397\n",
      "4 6\n",
      "39398\n",
      "0 5\n",
      "40085\n",
      "4 2\n",
      "41697\n",
      "4 2\n",
      "42617\n",
      "8 14\n",
      "44731\n",
      "2 0\n",
      "44732\n",
      "4 0\n",
      "46330\n",
      "0 2\n",
      "46532\n",
      "12 8\n",
      "46553\n",
      "2 0\n",
      "46554\n",
      "8 0\n",
      "47295\n",
      "14 2\n",
      "47867\n",
      "4 14\n",
      "47880\n",
      "2 0\n",
      "47934\n",
      "0 6\n",
      "47935\n",
      "0 2\n",
      "47941\n",
      "0 2\n",
      "48050\n",
      "2 4\n",
      "50158\n",
      "0 2\n",
      "53032\n",
      "2 0\n",
      "53054\n",
      "0 6\n",
      "53055\n",
      "6 2\n",
      "53150\n",
      "2 0\n",
      "53161\n",
      "6 0\n",
      "53264\n",
      "2 4\n",
      "53282\n",
      "2 4\n",
      "53301\n",
      "2 6\n",
      "53333\n",
      "4 0\n",
      "53399\n",
      "0 2\n",
      "53400\n",
      "2 3\n",
      "54616\n",
      "3 1\n",
      "57500\n",
      "2 4\n",
      "57501\n",
      "2 4\n",
      "57516\n",
      "10 12\n",
      "57648\n",
      "0 8\n",
      "57726\n",
      "2 0\n",
      "57727\n",
      "4 0\n",
      "57737\n",
      "2 0\n",
      "57747\n",
      "4 0\n",
      "57805\n",
      "0 2\n",
      "57860\n",
      "2 0\n",
      "57881\n",
      "6 0\n",
      "57906\n",
      "6 0\n",
      "57907\n",
      "4 0\n",
      "57933\n",
      "1 0\n",
      "57953\n",
      "2 0\n",
      "57955\n",
      "4 0\n",
      "57960\n",
      "0 4\n",
      "58029\n",
      "6 0\n",
      "58049\n",
      "2 0\n",
      "58064\n",
      "0 6\n",
      "58065\n",
      "8 4\n",
      "58066\n",
      "4 2\n",
      "59314\n",
      "2 0\n",
      "59356\n",
      "5 4\n",
      "59416\n",
      "2 0\n",
      "59418\n",
      "0 4\n",
      "59422\n",
      "2 0\n",
      "60619\n",
      "4 2\n",
      "63535\n",
      "0 8\n",
      "63547\n",
      "5 4\n",
      "64649\n",
      "2 0\n",
      "66036\n",
      "0 2\n",
      "66037\n",
      "6 0\n",
      "66038\n",
      "2 5\n",
      "66213\n",
      "2 14\n",
      "68631\n",
      "2 4\n",
      "68662\n",
      "0 8\n",
      "68801\n",
      "4 0\n",
      "68828\n",
      "4 0\n",
      "68903\n",
      "0 2\n",
      "68910\n",
      "2 4\n",
      "68924\n",
      "8 10\n",
      "68925\n",
      "0 2\n",
      "68931\n",
      "2 4\n",
      "68939\n",
      "12 8\n",
      "68940\n",
      "6 0\n",
      "69012\n",
      "0 2\n",
      "69098\n",
      "0 2\n",
      "72645\n",
      "0 2\n",
      "73429\n",
      "4 0\n",
      "73507\n",
      "0 6\n",
      "73508\n",
      "0 1\n",
      "73592\n",
      "14 0\n",
      "73641\n",
      "14 0\n",
      "73709\n",
      "0 4\n",
      "73710\n",
      "6 14\n",
      "73711\n",
      "4 0\n",
      "76752\n",
      "8 14\n",
      "76879\n",
      "0 3\n",
      "79185\n",
      "0 14\n",
      "79186\n",
      "2 14\n",
      "80530\n",
      "0 4\n",
      "80531\n",
      "6 14\n",
      "80532\n",
      "4 0\n",
      "83134\n",
      "2 0\n",
      "83135\n",
      "4 0\n",
      "83164\n",
      "3 5\n",
      "84085\n",
      "2 4\n",
      "85138\n",
      "0 4\n",
      "85139\n",
      "2 4\n",
      "85986\n",
      "14 2\n",
      "86189\n",
      "2 0\n",
      "87691\n",
      "2 4\n",
      "87697\n",
      "2 0\n",
      "89175\n",
      "0 2\n",
      "89177\n",
      "2 0\n",
      "90101\n",
      "0 2\n",
      "97320\n",
      "6 0\n",
      "99525\n",
      "0 2\n",
      "100376\n",
      "6 0\n",
      "100407\n",
      "0 2\n",
      "100409\n",
      "2 0\n",
      "100417\n",
      "0 2\n",
      "100460\n",
      "2 0\n",
      "101672\n",
      "6 0\n",
      "101948\n",
      "12 8\n",
      "107002\n",
      "3 5\n",
      "107548\n",
      "0 6\n",
      "107585\n",
      "0 4\n",
      "109942\n",
      "14 2\n",
      "111781\n",
      "8 4\n",
      "111789\n",
      "0 2\n",
      "112214\n",
      "2 0\n",
      "116274\n",
      "4 0\n",
      "116669\n",
      "4 0\n",
      "116758\n",
      "4 0\n",
      "119384\n",
      "4 5\n",
      "121378\n",
      "2 6\n",
      "123089\n",
      "0 6\n",
      "123889\n",
      "2 4\n",
      "126524\n",
      "8 7\n",
      "129157\n",
      "0 2\n",
      "131933\n",
      "0 2\n",
      "131953\n",
      "2 0\n",
      "132881\n",
      "0 4\n",
      "133311\n",
      "14 2\n",
      "134161\n",
      "0 4\n",
      "134280\n",
      "0 14\n",
      "134281\n",
      "5 0\n",
      "135793\n",
      "7 8\n",
      "138132\n",
      "2 0\n",
      "138146\n",
      "2 0\n",
      "138465\n",
      "0 4\n",
      "138732\n",
      "3 0\n",
      "138771\n",
      "0 6\n",
      "138772\n",
      "0 2\n",
      "140001\n",
      "0 2\n",
      "140003\n",
      "2 0\n",
      "140228\n",
      "14 2\n",
      "140269\n",
      "8 12\n",
      "140271\n",
      "2 0\n",
      "140277\n",
      "2 0\n",
      "140330\n",
      "1 0\n",
      "140362\n",
      "2 0\n",
      "140363\n",
      "6 0\n",
      "140371\n",
      "0 2\n",
      "140376\n",
      "0 2\n",
      "142545\n",
      "0 6\n",
      "142546\n",
      "0 4\n",
      "142549\n",
      "7 8\n",
      "142559\n",
      "0 4\n",
      "142566\n",
      "0 4\n",
      "142569\n",
      "7 8\n",
      "142571\n",
      "0 4\n",
      "142577\n",
      "0 4\n",
      "142804\n",
      "14 4\n",
      "142910\n",
      "0 14\n",
      "143400\n",
      "0 4\n",
      "146048\n",
      "14 0\n",
      "147202\n",
      "14 4\n",
      "148181\n",
      "6 0\n",
      "150458\n",
      "4 6\n",
      "150849\n",
      "0 2\n",
      "150851\n",
      "2 0\n",
      "150874\n",
      "0 2\n",
      "152533\n",
      "2 0\n",
      "152534\n",
      "14 0\n",
      "152535\n",
      "0 8\n",
      "154571\n",
      "2 0\n",
      "154572\n",
      "0 6\n",
      "154573\n",
      "12 2\n",
      "154580\n",
      "2 0\n",
      "154589\n",
      "0 1\n",
      "154624\n",
      "13 12\n",
      "154628\n",
      "3 5\n",
      "154634\n",
      "2 4\n",
      "154635\n",
      "2 4\n",
      "154650\n",
      "2 0\n",
      "157344\n",
      "0 2\n",
      "157383\n",
      "0 4\n",
      "157576\n",
      "2 0\n",
      "163649\n",
      "2 3\n",
      "163750\n",
      "2 3\n",
      "164206\n",
      "2 0\n",
      "164207\n",
      "4 0\n",
      "164815\n",
      "10 8\n",
      "164850\n",
      "2 0\n",
      "164852\n",
      "0 4\n",
      "164854\n",
      "4 0\n",
      "164855\n",
      "0 6\n",
      "164856\n",
      "2 0\n",
      "165241\n",
      "8 14\n",
      "167206\n",
      "2 0\n",
      "170436\n",
      "3 0\n",
      "170447\n",
      "6 2\n",
      "170449\n",
      "0 4\n",
      "170531\n",
      "0 14\n",
      "171788\n",
      "0 4\n",
      "173839\n",
      "1 3\n",
      "178542\n",
      "4 0\n",
      "178564\n",
      "4 0\n",
      "181286\n",
      "7 8\n",
      "181402\n",
      "7 8\n",
      "181413\n",
      "7 12\n",
      "181426\n",
      "7 8\n",
      "181429\n",
      "7 8\n",
      "181438\n",
      "7 8\n",
      "181480\n",
      "7 10\n",
      "181604\n",
      "7 8\n",
      "181616\n",
      "7 8\n",
      "181679\n",
      "7 8\n",
      "181710\n",
      "7 8\n",
      "181738\n",
      "7 8\n",
      "181780\n",
      "2 4\n",
      "181792\n",
      "7 12\n",
      "181810\n",
      "7 12\n",
      "181830\n",
      "7 12\n",
      "181852\n",
      "7 12\n",
      "181869\n",
      "3 2\n",
      "181870\n",
      "0 4\n",
      "181871\n",
      "0 6\n",
      "181872\n",
      "3 5\n",
      "181878\n",
      "3 5\n",
      "181883\n",
      "7 12\n",
      "181901\n",
      "1 3\n",
      "181909\n",
      "3 1\n",
      "181912\n",
      "7 8\n",
      "181922\n",
      "7 8\n",
      "181925\n",
      "7 8\n",
      "181946\n",
      "7 8\n",
      "181972\n",
      "7 8\n",
      "181994\n",
      "0 4\n",
      "181995\n",
      "0 2\n",
      "182017\n",
      "7 8\n",
      "182052\n",
      "7 8\n",
      "182056\n",
      "7 8\n",
      "182060\n",
      "7 8\n",
      "182069\n",
      "7 8\n",
      "182083\n",
      "7 13\n",
      "182091\n",
      "7 13\n",
      "182094\n",
      "7 8\n",
      "182138\n",
      "7 8\n",
      "182148\n",
      "7 8\n",
      "182153\n",
      "7 8\n",
      "182157\n",
      "2 0\n",
      "182159\n",
      "7 10\n",
      "182179\n",
      "7 12\n",
      "182193\n",
      "7 11\n",
      "182198\n",
      "5 2\n",
      "182203\n",
      "3 5\n",
      "182222\n",
      "4 0\n",
      "182224\n",
      "2 4\n",
      "182225\n",
      "2 4\n",
      "182235\n",
      "7 8\n",
      "182240\n",
      "14 4\n",
      "182249\n",
      "7 8\n",
      "182252\n",
      "7 8\n",
      "182256\n",
      "7 8\n",
      "182264\n",
      "7 8\n",
      "182268\n",
      "7 8\n",
      "182272\n",
      "7 8\n",
      "182280\n",
      "7 8\n",
      "182300\n",
      "7 11\n",
      "182302\n",
      "7 8\n",
      "182533\n",
      "7 0\n",
      "182881\n",
      "2 4\n",
      "182893\n",
      "7 8\n",
      "182909\n",
      "7 8\n",
      "182925\n",
      "7 8\n",
      "182929\n",
      "0 4\n",
      "182933\n",
      "2 4\n",
      "184581\n",
      "0 4\n",
      "184582\n",
      "6 14\n",
      "184583\n",
      "3 0\n",
      "185644\n",
      "2 0\n",
      "186342\n",
      "4 0\n",
      "186343\n",
      "6 0\n",
      "186344\n",
      "3 0\n",
      "187859\n",
      "0 2\n",
      "187870\n",
      "0 2\n",
      "187903\n",
      "0 5\n",
      "188876\n",
      "0 6\n",
      "189070\n",
      "2 4\n",
      "189690\n",
      "0 2\n",
      "193377\n",
      "0 2\n",
      "193378\n",
      "6 4\n",
      "195612\n",
      "4 0\n",
      "202321\n",
      "2 0\n",
      "202322\n",
      "4 6\n",
      "203171\n",
      "14 2\n",
      "203884\n",
      "0 4\n",
      "204486\n",
      "14 2\n",
      "205387\n",
      "2 0\n",
      "205611\n",
      "4 14\n",
      "205991\n",
      "14 2\n",
      "209736\n",
      "0 4\n",
      "209737\n",
      "0 1\n",
      "209966\n",
      "0 4\n",
      "210018\n",
      "10 8\n",
      "210019\n",
      "0 2\n",
      "210023\n",
      "4 0\n",
      "210024\n",
      "4 2\n",
      "210071\n",
      "0 4\n",
      "212123\n",
      "0 2\n",
      "212760\n",
      "14 2\n",
      "213962\n",
      "0 2\n",
      "213963\n",
      "6 0\n",
      "213964\n",
      "2 8\n",
      "221367\n",
      "14 0\n",
      "221398\n",
      "4 2\n",
      "221399\n",
      "4 2\n",
      "221406\n",
      "0 3\n",
      "221455\n",
      "3 1\n",
      "221520\n",
      "2 0\n",
      "221622\n",
      "4 0\n",
      "225081\n",
      "0 4\n",
      "225802\n",
      "4 14\n",
      "230074\n",
      "0 2\n",
      "230084\n",
      "4 0\n",
      "230104\n",
      "6 0\n",
      "230120\n",
      "0 4\n",
      "230123\n",
      "4 6\n",
      "230134\n",
      "6 4\n",
      "230143\n",
      "6 0\n",
      "230153\n",
      "4 0\n",
      "230818\n",
      "2 0\n",
      "230823\n",
      "0 2\n",
      "230836\n",
      "11 10\n",
      "230839\n",
      "2 4\n",
      "230854\n",
      "2 0\n",
      "230917\n",
      "2 0\n",
      "230922\n",
      "0 2\n",
      "230935\n",
      "11 10\n",
      "230938\n",
      "2 4\n",
      "230953\n",
      "2 0\n",
      "230984\n",
      "4 14\n",
      "231003\n",
      "2 0\n",
      "231004\n",
      "6 4\n",
      "231009\n",
      "0 2\n",
      "231012\n",
      "0 2\n",
      "231025\n",
      "11 10\n",
      "231028\n",
      "2 4\n",
      "231034\n",
      "0 2\n",
      "231043\n",
      "2 4\n",
      "231063\n",
      "0 4\n",
      "231065\n",
      "4 14\n",
      "231073\n",
      "2 0\n",
      "231103\n",
      "0 4\n",
      "231110\n",
      "0 2\n",
      "231112\n",
      "2 0\n",
      "231116\n",
      "0 2\n",
      "231134\n",
      "2 0\n",
      "232995\n",
      "4 0\n",
      "233118\n",
      "0 4\n",
      "238876\n",
      "2 3\n",
      "239992\n",
      "0 2\n",
      "239994\n",
      "0 4\n",
      "240652\n",
      "2 14\n",
      "240699\n",
      "6 0\n",
      "240700\n",
      "2 6\n",
      "241029\n",
      "0 4\n",
      "241705\n",
      "0 6\n",
      "241712\n",
      "0 4\n",
      "241835\n",
      "14 4\n",
      "242071\n",
      "0 4\n",
      "243771\n",
      "6 4\n",
      "250108\n",
      "0 4\n",
      "250109\n",
      "0 3\n",
      "250186\n",
      "8 11\n",
      "250242\n",
      "8 13\n",
      "255693\n",
      "0 4\n",
      "256477\n",
      "0 2\n",
      "256479\n",
      "4 0\n",
      "257991\n",
      "8 14\n",
      "260736\n",
      "0 4\n",
      "260737\n",
      "2 4\n",
      "260991\n",
      "0 4\n",
      "262646\n",
      "2 6\n",
      "262689\n",
      "0 6\n",
      "262693\n",
      "6 0\n",
      "262712\n",
      "0 2\n",
      "262713\n",
      "0 4\n",
      "262715\n",
      "0 4\n",
      "262716\n",
      "8 12\n",
      "262746\n",
      "6 0\n",
      "262747\n",
      "4 10\n",
      "262748\n",
      "4 2\n",
      "262764\n",
      "2 0\n",
      "262790\n",
      "14 11\n",
      "262814\n",
      "4 0\n",
      "262815\n",
      "6 2\n",
      "262824\n",
      "4 0\n",
      "262826\n",
      "5 4\n",
      "262827\n",
      "2 0\n",
      "262829\n",
      "8 4\n",
      "262830\n",
      "4 5\n",
      "262876\n",
      "0 2\n",
      "262877\n",
      "6 14\n",
      "262948\n",
      "6 2\n",
      "262952\n",
      "0 8\n",
      "262953\n",
      "6 2\n",
      "263372\n",
      "2 0\n",
      "263625\n",
      "4 5\n",
      "263626\n",
      "2 0\n",
      "263627\n",
      "6 0\n",
      "263628\n",
      "4 0\n",
      "264429\n",
      "2 4\n",
      "265770\n",
      "14 2\n",
      "267009\n",
      "4 0\n",
      "267010\n",
      "0 8\n",
      "269562\n",
      "0 2\n",
      "269563\n",
      "6 0\n",
      "269564\n",
      "2 8\n",
      "273110\n",
      "0 2\n",
      "273608\n",
      "14 0\n",
      "276896\n",
      "0 2\n",
      "276902\n",
      "4 0\n",
      "276907\n",
      "0 6\n",
      "276943\n",
      "2 0\n",
      "276944\n",
      "0 4\n",
      "276945\n",
      "8 0\n",
      "276970\n",
      "4 0\n",
      "276993\n",
      "6 0\n",
      "276997\n",
      "0 6\n",
      "276998\n",
      "6 2\n",
      "277078\n",
      "0 2\n",
      "277101\n",
      "1 5\n",
      "277147\n",
      "2 0\n",
      "277148\n",
      "8 0\n",
      "277173\n",
      "2 0\n",
      "277175\n",
      "0 4\n",
      "280014\n",
      "14 2\n",
      "281843\n",
      "0 2\n",
      "283508\n",
      "2 0\n",
      "284191\n",
      "0 6\n",
      "285647\n",
      "0 2\n",
      "286318\n",
      "2 4\n",
      "286335\n",
      "2 4\n",
      "290490\n",
      "2 0\n",
      "290577\n",
      "14 2\n",
      "290982\n",
      "2 0\n",
      "291968\n",
      "0 2\n",
      "294998\n",
      "14 2\n",
      "295824\n",
      "4 5\n",
      "295852\n",
      "0 6\n",
      "295860\n",
      "2 0\n",
      "295862\n",
      "4 0\n",
      "295925\n",
      "2 4\n",
      "296442\n",
      "0 4\n",
      "301971\n",
      "0 6\n",
      "302910\n",
      "0 2\n",
      "305196\n",
      "8 14\n",
      "305859\n",
      "10 7\n",
      "306112\n",
      "0 4\n",
      "307815\n",
      "8 14\n",
      "308008\n",
      "2 0\n",
      "308009\n",
      "0 6\n",
      "308010\n",
      "8 2\n",
      "308801\n",
      "0 6\n",
      "308802\n",
      "0 4\n",
      "309094\n",
      "4 2\n",
      "309095\n",
      "4 2\n",
      "310092\n",
      "8 14\n",
      "310108\n",
      "8 14\n",
      "311319\n",
      "0 4\n",
      "313560\n",
      "14 2\n",
      "314624\n",
      "8 10\n",
      "314697\n",
      "6 8\n",
      "314713\n",
      "0 2\n",
      "314720\n",
      "0 4\n",
      "314834\n",
      "4 0\n",
      "315012\n",
      "14 2\n",
      "316264\n",
      "2 0\n",
      "316573\n",
      "14 2\n",
      "319531\n",
      "10 7\n",
      "321578\n",
      "4 2\n",
      "324821\n",
      "14 2\n",
      "324822\n",
      "14 2\n",
      "325120\n",
      "10 8\n",
      "327073\n",
      "2 0\n",
      "327222\n",
      "2 0\n",
      "327233\n",
      "4 5\n",
      "328001\n",
      "3 1\n",
      "329130\n",
      "6 0\n",
      "329145\n",
      "0 2\n",
      "329791\n",
      "2 4\n",
      "329806\n",
      "6 0\n",
      "330184\n",
      "0 2\n",
      "330187\n",
      "2 0\n",
      "330191\n",
      "2 0\n",
      "330193\n",
      "12 8\n",
      "330227\n",
      "0 14\n",
      "330268\n",
      "4 0\n",
      "330269\n",
      "0 6\n",
      "330276\n",
      "4 0\n",
      "330331\n",
      "0 6\n",
      "330332\n",
      "6 0\n",
      "331199\n",
      "0 4\n",
      "335975\n",
      "14 4\n",
      "339301\n",
      "0 2\n",
      "340988\n",
      "0 4\n",
      "340997\n",
      "0 4\n",
      "341050\n",
      "2 4\n",
      "342832\n",
      "0 2\n",
      "348252\n",
      "12 10\n",
      "348253\n",
      "4 2\n",
      "350587\n",
      "2 0\n",
      "352770\n",
      "14 0\n",
      "352800\n",
      "14 2\n",
      "354134\n",
      "14 2\n",
      "354537\n",
      "0 2\n",
      "355611\n",
      "2 4\n",
      "355612\n",
      "2 4\n",
      "355856\n",
      "0 4\n",
      "356083\n",
      "14 4\n",
      "357737\n",
      "14 2\n",
      "358392\n",
      "14 0\n",
      "358410\n",
      "0 14\n",
      "358418\n",
      "6 14\n",
      "359125\n",
      "4 14\n",
      "359330\n",
      "4 14\n",
      "359342\n",
      "0 8\n",
      "363500\n",
      "4 14\n",
      "364036\n",
      "8 14\n",
      "365254\n",
      "14 2\n",
      "365871\n",
      "0 8\n",
      "365875\n",
      "3 2\n",
      "366888\n",
      "0 4\n",
      "367342\n",
      "4 2\n",
      "367686\n",
      "10 7\n",
      "367963\n",
      "2 4\n",
      "369533\n",
      "0 2\n",
      "371263\n",
      "2 4\n",
      "372518\n",
      "14 2\n",
      "373205\n",
      "14 4\n",
      "373838\n",
      "2 4\n",
      "376295\n",
      "4 0\n",
      "377991\n",
      "0 2\n",
      "378941\n",
      "2 0\n",
      "378943\n",
      "0 2\n",
      "379072\n",
      "0 2\n",
      "379924\n",
      "0 4\n",
      "383422\n",
      "14 2\n",
      "384001\n",
      "0 2\n",
      "384056\n",
      "2 0\n",
      "385752\n",
      "4 6\n",
      "385884\n",
      "4 0\n",
      "385885\n",
      "0 14\n",
      "385889\n",
      "14 0\n",
      "385890\n",
      "0 14\n",
      "385895\n",
      "12 8\n",
      "385896\n",
      "14 12\n",
      "387005\n",
      "2 0\n",
      "387675\n",
      "0 2\n",
      "389876\n",
      "2 6\n",
      "391037\n",
      "3 5\n",
      "392881\n",
      "0 6\n",
      "392882\n",
      "0 5\n",
      "392888\n",
      "0 2\n",
      "392915\n",
      "2 0\n",
      "392916\n",
      "3 2\n",
      "392927\n",
      "3 5\n",
      "396691\n",
      "0 2\n",
      "397439\n",
      "0 4\n",
      "397443\n",
      "2 4\n",
      "399184\n",
      "10 2\n",
      "399198\n",
      "6 0\n",
      "399260\n",
      "2 4\n",
      "399666\n",
      "2 6\n",
      "399700\n",
      "2 4\n",
      "400660\n",
      "4 14\n",
      "401102\n",
      "6 2\n",
      "401138\n",
      "2 4\n",
      "403360\n",
      "4 2\n",
      "403480\n",
      "0 2\n",
      "407837\n",
      "10 8\n",
      "408463\n",
      "2 0\n",
      "408464\n",
      "4 0\n",
      "408773\n",
      "0 2\n",
      "408775\n",
      "4 0\n",
      "408833\n",
      "0 2\n",
      "408836\n",
      "4 0\n",
      "409365\n",
      "4 14\n",
      "410047\n",
      "0 4\n",
      "410053\n",
      "2 4\n",
      "410056\n",
      "4 0\n",
      "412262\n",
      "10 7\n",
      "414999\n",
      "2 0\n",
      "418696\n",
      "2 4\n",
      "421245\n",
      "0 14\n",
      "421246\n",
      "12 4\n",
      "421248\n",
      "3 1\n",
      "421251\n",
      "0 4\n",
      "423621\n",
      "4 0\n",
      "423651\n",
      "0 2\n",
      "423655\n",
      "4 0\n",
      "423758\n",
      "4 0\n",
      "423761\n",
      "0 2\n",
      "423765\n",
      "4 0\n",
      "423783\n",
      "3 2\n",
      "423787\n",
      "3 5\n",
      "423815\n",
      "0 6\n",
      "423816\n",
      "6 4\n",
      "423829\n",
      "2 0\n",
      "423830\n",
      "0 6\n",
      "423831\n",
      "8 0\n",
      "423832\n",
      "0 4\n",
      "423862\n",
      "2 4\n",
      "Accuracy: 99.85%\n"
     ]
    }
   ],
   "source": [
    "gold_test = pd.read_csv('clean_out/test_gold.csv',index_col=0)\n",
    "sys_test = pd.read_csv('results/letter_haraka.csv',index_col=0)\n",
    "# Accuracy per letter\n",
    "# print(gold_test.head())\n",
    "# print(sys_test.head())   \n",
    "# print(gold_test.iloc[0]['label'])\n",
    "\n",
    "correct = 0\n",
    "total = len(gold_test)\n",
    "for i in range(total):\n",
    "    # print(gold_test[i][0], sys_test[i][0])\n",
    "    if gold_test.iloc[i]['label'] == sys_test.iloc[i]['label']:\n",
    "        correct +=1\n",
    "    else:\n",
    "        print(i)\n",
    "        print(gold_test.iloc[i]['label'], sys_test.iloc[i]['label'])\n",
    "    \n",
    "print(\"Accuracy: %.2f%%\" % (100.0 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
