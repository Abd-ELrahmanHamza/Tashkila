{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# to reload modules automatically without having to restart the kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from letters_dataset import LettersDataset\n",
    "import torch.nn as nn\n",
    "from train_collections import *\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.isri import ISRIStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"clean_out/merged_unsplited.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "stemmer = ISRIStemmer()\n",
    "# replace , and - with space\n",
    "text = text.replace(\"،\", \"\")\n",
    "text = text.replace(\"-\", \"\")\n",
    "text = text.split(\"\\n\")\n",
    "\n",
    "text = [sentence.split() for sentence in text]\n",
    "lengths = [len(sentence) for sentence in text]\n",
    "lengths = np.cumsum(lengths)\n",
    "text = [[stemmer.stem(word) for word in sentence] for sentence in text]\n",
    "\n",
    "vocab = set([word for sentence in text for word in sentence] + [\"<S>\", \"</S>\", \"<UNK>\"])\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for i, word in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and training parameters\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 417\n"
     ]
    }
   ],
   "source": [
    "# load train data\n",
    "dataset = LettersDataset(device=device,word2idx=word2idx, return_sent_emb=False)\n",
    "dataset =LettersDataset('clean_out/X_train.csv', 'clean_out/y_train.csv',val_mode=False, device=device) \n",
    "loader = data.DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "sample = next(iter(loader))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_chars:  41\n",
      "n_harakat:  15\n"
     ]
    }
   ],
   "source": [
    "n_chars = dataset.get_input_vocab_size()\n",
    "n_harakat = dataset.get_output_vocab_size()\n",
    "print(\"n_chars: \", n_chars)\n",
    "print(\"n_harakat: \", n_harakat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }, filename)\n",
    "    \n",
    "    \n",
    "def load_checkpoint(model, optimizer, filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch'] + 1\n",
    "    loss = checkpoint['loss']\n",
    "    return epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Accio import Accio\n",
    "model = Accio(input_size=n_chars, output_size=n_harakat,device=device).to(device)\n",
    "# model.load_state_dict(torch.load(\"models/Accio_deep_9.pth\"))\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "_ = load_checkpoint(model, optimizer, \"models/Accio_deep_19.pth\")\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 0: Loss = 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:25,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 100: Loss = 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:44,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Cross-entropy: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 0: Loss = 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:25,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 100: Loss = 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:44,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Cross-entropy: 0.0350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, batch 0: Loss = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:25,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, batch 100: Loss = 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:44,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Cross-entropy: 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, batch 0: Loss = 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:26,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, batch 100: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:47,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Cross-entropy: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, batch 0: Loss = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:26,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, batch 100: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:46,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Cross-entropy: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, batch 0: Loss = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:25,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, batch 100: Loss = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:44,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Cross-entropy: 0.0333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, batch 0: Loss = 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:25,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, batch 100: Loss = 0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:45,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Cross-entropy: 0.3146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, batch 0: Loss = 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:26,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, batch 100: Loss = 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:45,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Cross-entropy: 0.1292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, batch 0: Loss = 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:25,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, batch 100: Loss = 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:44,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Cross-entropy: 0.0521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, batch 0: Loss = 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:26,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, batch 100: Loss = 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:45,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Cross-entropy: 0.0276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, batch 0: Loss = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:25,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, batch 100: Loss = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:45,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Cross-entropy: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, batch 0: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:26,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, batch 100: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:45,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Cross-entropy: 0.0090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, batch 0: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:26,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, batch 100: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:45,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Cross-entropy: 0.0067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, batch 0: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:26,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, batch 100: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:46,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Cross-entropy: 0.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, batch 0: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:26,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, batch 100: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:45,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Cross-entropy: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, batch 0: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:26,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, batch 100: Loss = 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:46,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Cross-entropy: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, batch 0: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:26,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, batch 100: Loss = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:46,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Cross-entropy: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, batch 0: Loss = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:25,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, batch 100: Loss = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:45,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Cross-entropy: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, batch 0: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:25,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, batch 100: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:45,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Cross-entropy: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, batch 0: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:26,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, batch 100: Loss = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:45,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Cross-entropy: 0.0059\n"
     ]
    }
   ],
   "source": [
    "num_batches = len(loader)\n",
    "print(\"Number of batches:\", num_batches)\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "for epoch in range(n_epochs):\n",
    "    torch.cuda.empty_cache()  # Clear CUDA cache\n",
    "    \n",
    "    model.train()\n",
    "    for i, (X_batch, y_batch) in tqdm(enumerate(loader)):\n",
    "        y_pred = ''\n",
    "        # y_pred = model(X_batch)['diacritics']\n",
    "        y_pred = model(X_batch)\n",
    "        # we transpose because the loss function expects the second dimension to be the classes\n",
    "        # y_pred is now (batch_size, n_classes, seq_len)\n",
    "        y_pred = y_pred.transpose(1, 2)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch %d, batch %d: Loss = %.4f\" % (epoch, i, loss))\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    \n",
    "    # torch.save(model.state_dict(), f'models/Accio_deept_{epoch}.pth')\n",
    "    save_checkpoint(model, optimizer, epoch, 0, f'models/Accio_deep_{epoch}.pth')\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (X_batch, y_batch) in loader:\n",
    "            # y_pred = model(X_batch)['diacritics']\n",
    "            y_pred = model(X_batch)\n",
    "            y_pred = y_pred.transpose(1, 2)\n",
    "            loss += loss_fn(y_pred, y_batch)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = model.state_dict()\n",
    "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 1129\n",
      "{'ا': 0, 'ب': 1, 'ت': 2, 'ث': 3, 'ج': 4, 'ح': 5, 'خ': 6, 'د': 7, 'ذ': 8, 'ر': 9, 'ز': 10, 'س': 11, 'ش': 12, 'ص': 13, 'ض': 14, 'ط': 15, 'ظ': 16, 'ع': 17, 'غ': 18, 'ف': 19, 'ق': 20, 'ك': 21, 'ل': 22, 'م': 23, 'ن': 24, 'ه': 25, 'و': 26, 'ي': 27, 'ى': 28, 'ة': 29, 'آ': 30, 'أ': 31, 'إ': 32, 'ء': 33, 'ؤ': 34, 'ئ': 35, ' ': 36, '،': 37, '-': 38, '<pad>': 39, '<unk>': 40}\n"
     ]
    }
   ],
   "source": [
    "val_dataset = LettersDataset('clean_out/X_val.csv', 'clean_out/y_val.csv', val_mode=True, device=device)\n",
    "\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "print(val_dataset.char_encoder.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluaate accuracy on validation set\n",
    "model.eval()\n",
    "letter_haraka = []\n",
    "with torch.no_grad():\n",
    "    for (X_batch, y_batch) in val_loader:\n",
    "        # y_pred = model(X_batch)['diacritics']\n",
    "        y_pred = model(X_batch)\n",
    "        # we transpose because the loss function expects the second dimension to be the classes\n",
    "        # y_pred is now (batch_size, n_classes, seq_len)\n",
    "        y_pred = y_pred.transpose(1, 2)\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        # Count only non-padding characters\n",
    "        for x, y in zip(X_batch, predicted):\n",
    "            for xx, yy in zip(x, y):\n",
    "                # we reached the end of the sentence\n",
    "                # print(xx.item())\n",
    "                # print(val_dataset.char_encoder.get_pad_id())\n",
    "                # print(val_dataset.char_encoder.get_id_by_token(UNK_TOKEN))\n",
    "                if xx.item() == val_dataset.char_encoder.get_pad_id():\n",
    "                    break\n",
    "                ll = val_dataset.char_encoder.is_arabic_letter(xx.item())\n",
    "                if ll:\n",
    "                    letter_haraka.append([ll, yy.item()])\n",
    "\n",
    "# save ID,Label pairs in a csv file\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(letter_haraka, columns=['letter', 'label'])\n",
    "df.to_csv('./results/letter_haraka.csv', index=True, index_label='ID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.51%\n"
     ]
    }
   ],
   "source": [
    "gold_val = pd.read_csv('clean_out/val_gold.csv', index_col=0)\n",
    "sys_val = pd.read_csv('results/letter_haraka.csv', index_col=0)\n",
    "# Accuracy per letter\n",
    "# print(gold_val.head())\n",
    "# print(sys_val.head())   \n",
    "# print(gold_val.iloc[0]['label'])\n",
    "\n",
    "correct = 0\n",
    "total = len(gold_val)\n",
    "for i in range(total):\n",
    "    # print(gold_val[i][0], sys_val[i][0])\n",
    "    correct += (gold_val.iloc[i]['label'] == sys_val.iloc[i]['label'])\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (100.0 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model \n",
    "# torch.save(model, 'models/lstm.pth')\n",
    "# save model state dict\n",
    "torch.save(model.state_dict(), 'models/Accio_4.pth')\n",
    "# load model state dict\n",
    "# model = BiLSTM()\n",
    "# model.load_state_dict(torch.load('models/bilstm.pth'))\n",
    "# load model\n",
    "# model = torch.load('models/___.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DER of the network on the validation set: 1.493498 %\n"
     ]
    }
   ],
   "source": [
    "print('DER of the network on the validation set: %f %%' % (100.0 * (1 - correct / total)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 1174\n",
      "{'ا': 0, 'ب': 1, 'ت': 2, 'ث': 3, 'ج': 4, 'ح': 5, 'خ': 6, 'د': 7, 'ذ': 8, 'ر': 9, 'ز': 10, 'س': 11, 'ش': 12, 'ص': 13, 'ض': 14, 'ط': 15, 'ظ': 16, 'ع': 17, 'غ': 18, 'ف': 19, 'ق': 20, 'ك': 21, 'ل': 22, 'م': 23, 'ن': 24, 'ه': 25, 'و': 26, 'ي': 27, 'ى': 28, 'ة': 29, 'آ': 30, 'أ': 31, 'إ': 32, 'ء': 33, 'ؤ': 34, 'ئ': 35, ' ': 36, '،': 37, '-': 38, '<pad>': 39, '<unk>': 40}\n"
     ]
    }
   ],
   "source": [
    "test_dataset = LettersDataset('clean_out/X_test.csv', 'clean_out/y_test.csv',val_mode=True, device=device)   \n",
    "val_loader = data.DataLoader(test_dataset,  batch_size=batch_size)\n",
    "print(test_dataset.char_encoder.word2idx)\n",
    "# evaluaate accuracy on validation set\n",
    "\n",
    "model.eval()\n",
    "letter_haraka = []\n",
    "with torch.no_grad():\n",
    "    for (X_batch,y_batch) in val_loader:\n",
    "        # y_pred = model(X_batch)['diacritics']\n",
    "        y_pred = model(X_batch)\n",
    "        # we transpose because the loss function expects the second dimension to be the classes\n",
    "        # y_pred is now (batch_size, n_classes, seq_len)\n",
    "        y_pred = y_pred.transpose(1, 2) \n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        # Count only non-padding characters\n",
    "        for x,y in zip(X_batch,predicted):\n",
    "            for xx,yy in zip(x,y):\n",
    "                # we reached the end of the sentence\n",
    "                # print(xx.item())\n",
    "                # print(test_dataset.char_encoder.get_pad_id())\n",
    "                # print(test_dataset.char_encoder.get_id_by_token(UNK_TOKEN))\n",
    "                if xx.item() == test_dataset.char_encoder.get_pad_id():\n",
    "                    break\n",
    "                ll = test_dataset.char_encoder.is_arabic_letter(xx.item())\n",
    "                if ll:\n",
    "                    letter_haraka.append([ll,yy.item()])\n",
    "\n",
    "# save ID,Label pairs in a csv file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(letter_haraka, columns=['letter','label'])\n",
    "df.to_csv('./results/letter_haraka.csv', index=True, index_label='ID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.88%\n"
     ]
    }
   ],
   "source": [
    "gold_test = pd.read_csv('clean_out/test_gold.csv',index_col=0)\n",
    "sys_test = pd.read_csv('results/letter_haraka.csv',index_col=0)\n",
    "# Accuracy per letter\n",
    "# print(gold_test.head())\n",
    "# print(sys_test.head())   \n",
    "# print(gold_test.iloc[0]['label'])\n",
    "\n",
    "correct = 0\n",
    "total = len(gold_test)\n",
    "for i in range(total):\n",
    "    # print(gold_test[i][0], sys_test[i][0])\n",
    "    if gold_test.iloc[i]['label'] == sys_test.iloc[i]['label']:\n",
    "        correct +=1\n",
    "    else:\n",
    "        pass\n",
    "        # print(i)\n",
    "        # print(gold_test.iloc[i]['label'], sys_test.iloc[i]['label'])\n",
    "    \n",
    "print(\"Accuracy: %.2f%%\" % (100.0 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 1174\n",
      "{'ا': 0, 'ب': 1, 'ت': 2, 'ث': 3, 'ج': 4, 'ح': 5, 'خ': 6, 'د': 7, 'ذ': 8, 'ر': 9, 'ز': 10, 'س': 11, 'ش': 12, 'ص': 13, 'ض': 14, 'ط': 15, 'ظ': 16, 'ع': 17, 'غ': 18, 'ف': 19, 'ق': 20, 'ك': 21, 'ل': 22, 'م': 23, 'ن': 24, 'ه': 25, 'و': 26, 'ي': 27, 'ى': 28, 'ة': 29, 'آ': 30, 'أ': 31, 'إ': 32, 'ء': 33, 'ؤ': 34, 'ئ': 35, ' ': 36, '،': 37, '-': 38, '<pad>': 39, '<unk>': 40}\n",
      "904\n",
      "5 3\n",
      "1244\n",
      "8 0\n",
      "1247\n",
      "0 6\n",
      "1248\n",
      "0 2\n",
      "3277\n",
      "2 14\n",
      "6215\n",
      "14 4\n",
      "7310\n",
      "0 2\n",
      "7510\n",
      "2 14\n",
      "11403\n",
      "14 0\n",
      "11579\n",
      "14 0\n",
      "11972\n",
      "2 14\n",
      "19336\n",
      "0 2\n",
      "19344\n",
      "0 6\n",
      "21971\n",
      "4 2\n",
      "24401\n",
      "0 4\n",
      "39404\n",
      "2 0\n",
      "39405\n",
      "4 6\n",
      "39406\n",
      "0 5\n",
      "40093\n",
      "4 2\n",
      "46534\n",
      "12 8\n",
      "46535\n",
      "0 5\n",
      "47570\n",
      "4 14\n",
      "47878\n",
      "4 14\n",
      "47890\n",
      "0 6\n",
      "47892\n",
      "8 0\n",
      "47945\n",
      "0 6\n",
      "47946\n",
      "0 4\n",
      "48009\n",
      "1 0\n",
      "48061\n",
      "2 4\n",
      "53051\n",
      "4 5\n",
      "53065\n",
      "0 6\n",
      "53066\n",
      "6 2\n",
      "53161\n",
      "2 0\n",
      "53169\n",
      "6 14\n",
      "53293\n",
      "2 0\n",
      "53352\n",
      "2 0\n",
      "53369\n",
      "0 4\n",
      "53370\n",
      "0 2\n",
      "53390\n",
      "0 6\n",
      "54629\n",
      "3 1\n",
      "57516\n",
      "2 4\n",
      "57517\n",
      "2 4\n",
      "57536\n",
      "0 2\n",
      "57538\n",
      "2 4\n",
      "57668\n",
      "4 0\n",
      "57742\n",
      "2 0\n",
      "57743\n",
      "4 0\n",
      "57763\n",
      "4 2\n",
      "57827\n",
      "4 0\n",
      "57865\n",
      "4 0\n",
      "57866\n",
      "4 2\n",
      "57922\n",
      "6 0\n",
      "57923\n",
      "4 0\n",
      "57949\n",
      "1 0\n",
      "57969\n",
      "2 0\n",
      "57971\n",
      "4 0\n",
      "57976\n",
      "0 2\n",
      "58065\n",
      "2 0\n",
      "58080\n",
      "0 6\n",
      "58082\n",
      "4 2\n",
      "59330\n",
      "2 0\n",
      "59372\n",
      "5 4\n",
      "59432\n",
      "2 0\n",
      "59434\n",
      "0 4\n",
      "59438\n",
      "2 0\n",
      "62081\n",
      "2 14\n",
      "66050\n",
      "4 0\n",
      "66054\n",
      "2 5\n",
      "66229\n",
      "2 14\n",
      "68647\n",
      "2 4\n",
      "68780\n",
      "0 2\n",
      "68784\n",
      "0 2\n",
      "68817\n",
      "4 0\n",
      "68829\n",
      "8 0\n",
      "68844\n",
      "4 0\n",
      "68919\n",
      "0 2\n",
      "68940\n",
      "8 10\n",
      "68941\n",
      "0 2\n",
      "68947\n",
      "2 4\n",
      "68955\n",
      "12 8\n",
      "68956\n",
      "6 0\n",
      "69014\n",
      "2 0\n",
      "69028\n",
      "0 2\n",
      "73522\n",
      "0 4\n",
      "73523\n",
      "0 6\n",
      "73524\n",
      "0 1\n",
      "73606\n",
      "2 5\n",
      "73655\n",
      "2 5\n",
      "73726\n",
      "6 14\n",
      "73727\n",
      "4 0\n",
      "80388\n",
      "4 0\n",
      "80389\n",
      "14 6\n",
      "80390\n",
      "0 4\n",
      "81342\n",
      "3 2\n",
      "83150\n",
      "2 0\n",
      "83151\n",
      "4 0\n",
      "87650\n",
      "4 6\n",
      "87659\n",
      "0 2\n",
      "87707\n",
      "0 2\n",
      "96191\n",
      "7 10\n",
      "97256\n",
      "4 0\n",
      "97281\n",
      "3 2\n",
      "97316\n",
      "0 6\n",
      "99546\n",
      "0 2\n",
      "100065\n",
      "4 0\n",
      "100066\n",
      "4 2\n",
      "100428\n",
      "0 2\n",
      "100430\n",
      "2 0\n",
      "100481\n",
      "2 0\n",
      "101430\n",
      "1 3\n",
      "101693\n",
      "6 0\n",
      "103556\n",
      "2 14\n",
      "107023\n",
      "3 5\n",
      "111802\n",
      "8 12\n",
      "111810\n",
      "0 2\n",
      "111825\n",
      "0 4\n",
      "119405\n",
      "4 5\n",
      "122459\n",
      "0 2\n",
      "122760\n",
      "5 0\n",
      "123128\n",
      "0 2\n",
      "123129\n",
      "6 14\n",
      "129178\n",
      "0 2\n",
      "138157\n",
      "2 0\n",
      "138163\n",
      "2 0\n",
      "138490\n",
      "0 4\n",
      "138755\n",
      "0 2\n",
      "138756\n",
      "6 4\n",
      "138757\n",
      "3 0\n",
      "140253\n",
      "14 2\n",
      "140294\n",
      "8 12\n",
      "140296\n",
      "2 0\n",
      "140302\n",
      "2 0\n",
      "140387\n",
      "2 0\n",
      "140388\n",
      "6 0\n",
      "142571\n",
      "0 6\n",
      "142572\n",
      "0 4\n",
      "142585\n",
      "0 4\n",
      "142591\n",
      "0 6\n",
      "142592\n",
      "0 4\n",
      "142595\n",
      "7 8\n",
      "142597\n",
      "0 4\n",
      "142603\n",
      "0 4\n",
      "142606\n",
      "0 6\n",
      "142607\n",
      "0 4\n",
      "154599\n",
      "0 6\n",
      "154600\n",
      "12 2\n",
      "154616\n",
      "0 1\n",
      "154635\n",
      "4 0\n",
      "154636\n",
      "14 6\n",
      "154651\n",
      "13 12\n",
      "154655\n",
      "3 5\n",
      "154661\n",
      "2 4\n",
      "154662\n",
      "2 4\n",
      "154679\n",
      "0 4\n",
      "164807\n",
      "2 3\n",
      "164814\n",
      "0 3\n",
      "164838\n",
      "8 0\n",
      "164842\n",
      "10 8\n",
      "164877\n",
      "2 0\n",
      "164879\n",
      "0 4\n",
      "164881\n",
      "4 0\n",
      "164882\n",
      "0 6\n",
      "167636\n",
      "4 0\n",
      "170443\n",
      "0 4\n",
      "170476\n",
      "6 2\n",
      "170486\n",
      "4 14\n",
      "170493\n",
      "4 5\n",
      "170497\n",
      "0 6\n",
      "170498\n",
      "0 5\n",
      "170555\n",
      "8 0\n",
      "171630\n",
      "0 14\n",
      "171749\n",
      "2 14\n",
      "181444\n",
      "7 12\n",
      "181457\n",
      "7 8\n",
      "181469\n",
      "7 8\n",
      "181511\n",
      "7 10\n",
      "181530\n",
      "7 0\n",
      "181544\n",
      "7 8\n",
      "181586\n",
      "7 8\n",
      "181589\n",
      "7 8\n",
      "181619\n",
      "7 8\n",
      "181635\n",
      "7 8\n",
      "181647\n",
      "7 8\n",
      "181672\n",
      "7 8\n",
      "181680\n",
      "7 8\n",
      "181687\n",
      "7 6\n",
      "181710\n",
      "7 8\n",
      "181741\n",
      "7 8\n",
      "181769\n",
      "7 8\n",
      "181818\n",
      "5 0\n",
      "181823\n",
      "7 12\n",
      "181841\n",
      "7 12\n",
      "181861\n",
      "7 12\n",
      "181883\n",
      "7 12\n",
      "181900\n",
      "3 2\n",
      "181901\n",
      "0 4\n",
      "181902\n",
      "0 6\n",
      "181914\n",
      "7 12\n",
      "181932\n",
      "1 3\n",
      "181940\n",
      "3 1\n",
      "181943\n",
      "7 8\n",
      "181953\n",
      "7 8\n",
      "181956\n",
      "7 8\n",
      "181977\n",
      "7 8\n",
      "182003\n",
      "7 8\n",
      "182025\n",
      "0 4\n",
      "182026\n",
      "0 2\n",
      "182048\n",
      "7 8\n",
      "182083\n",
      "7 8\n",
      "182087\n",
      "7 8\n",
      "182091\n",
      "7 8\n",
      "182100\n",
      "7 8\n",
      "182114\n",
      "7 13\n",
      "182122\n",
      "7 13\n",
      "182125\n",
      "7 8\n",
      "182169\n",
      "7 8\n",
      "182179\n",
      "7 8\n",
      "182184\n",
      "7 8\n",
      "182190\n",
      "7 10\n",
      "182210\n",
      "7 12\n",
      "182216\n",
      "5 3\n",
      "182224\n",
      "7 11\n",
      "182229\n",
      "5 2\n",
      "182266\n",
      "7 8\n",
      "182280\n",
      "7 8\n",
      "182283\n",
      "7 8\n",
      "182287\n",
      "7 8\n",
      "182295\n",
      "7 8\n",
      "182299\n",
      "7 8\n",
      "182303\n",
      "7 8\n",
      "182307\n",
      "7 8\n",
      "182311\n",
      "7 8\n",
      "182327\n",
      "7 8\n",
      "182331\n",
      "7 3\n",
      "182333\n",
      "7 8\n",
      "182897\n",
      "4 0\n",
      "182898\n",
      "4 2\n",
      "182907\n",
      "4 0\n",
      "182908\n",
      "4 2\n",
      "182940\n",
      "7 8\n",
      "182963\n",
      "2 4\n",
      "182964\n",
      "2 4\n",
      "183704\n",
      "6 2\n",
      "183705\n",
      "4 12\n",
      "184613\n",
      "0 4\n",
      "184614\n",
      "6 14\n",
      "184615\n",
      "3 0\n",
      "187891\n",
      "0 2\n",
      "187902\n",
      "0 2\n",
      "187935\n",
      "0 5\n",
      "188908\n",
      "0 6\n",
      "189132\n",
      "4 2\n",
      "189151\n",
      "4 2\n",
      "189722\n",
      "0 2\n",
      "193409\n",
      "0 2\n",
      "193410\n",
      "6 4\n",
      "193499\n",
      "2 14\n",
      "210001\n",
      "0 4\n",
      "210002\n",
      "2 4\n",
      "210053\n",
      "10 0\n",
      "210054\n",
      "0 2\n",
      "210059\n",
      "4 2\n",
      "210106\n",
      "0 4\n",
      "210107\n",
      "8 0\n",
      "211229\n",
      "4 0\n",
      "211230\n",
      "4 2\n",
      "211859\n",
      "2 14\n",
      "212158\n",
      "0 2\n",
      "213392\n",
      "2 14\n",
      "221405\n",
      "14 0\n",
      "221493\n",
      "3 1\n",
      "221545\n",
      "0 2\n",
      "221585\n",
      "0 2\n",
      "221586\n",
      "0 4\n",
      "221602\n",
      "0 2\n",
      "221660\n",
      "4 0\n",
      "224999\n",
      "4 2\n",
      "227186\n",
      "7 10\n",
      "227515\n",
      "5 3\n",
      "227524\n",
      "2 4\n",
      "230092\n",
      "2 0\n",
      "230112\n",
      "0 4\n",
      "230124\n",
      "2 4\n",
      "230141\n",
      "4 0\n",
      "230142\n",
      "6 0\n",
      "230180\n",
      "4 0\n",
      "230191\n",
      "4 0\n",
      "230192\n",
      "6 0\n",
      "230824\n",
      "4 14\n",
      "230825\n",
      "8 7\n",
      "230833\n",
      "0 2\n",
      "230845\n",
      "4 2\n",
      "230861\n",
      "0 2\n",
      "230877\n",
      "2 5\n",
      "230924\n",
      "8 7\n",
      "230932\n",
      "0 2\n",
      "230944\n",
      "4 2\n",
      "230960\n",
      "0 2\n",
      "230976\n",
      "2 5\n",
      "231031\n",
      "0 2\n",
      "231041\n",
      "2 0\n",
      "231066\n",
      "2 4\n",
      "231111\n",
      "2 0\n",
      "231138\n",
      "8 10\n",
      "231139\n",
      "6 2\n",
      "231141\n",
      "0 4\n",
      "231148\n",
      "0 2\n",
      "231150\n",
      "2 0\n",
      "231159\n",
      "4 14\n",
      "231172\n",
      "2 0\n",
      "233124\n",
      "2 0\n",
      "233156\n",
      "0 4\n",
      "240738\n",
      "2 14\n",
      "240739\n",
      "2 14\n",
      "241728\n",
      "4 0\n",
      "241743\n",
      "0 6\n",
      "241750\n",
      "0 4\n",
      "241834\n",
      "0 2\n",
      "241838\n",
      "0 4\n",
      "241839\n",
      "8 10\n",
      "241957\n",
      "2 1\n",
      "241958\n",
      "2 4\n",
      "241997\n",
      "0 2\n",
      "241998\n",
      "0 4\n",
      "242052\n",
      "0 2\n",
      "242110\n",
      "3 0\n",
      "246448\n",
      "4 14\n",
      "246457\n",
      "12 8\n",
      "246458\n",
      "14 6\n",
      "246467\n",
      "0 2\n",
      "250100\n",
      "3 0\n",
      "250147\n",
      "0 4\n",
      "250148\n",
      "0 3\n",
      "250285\n",
      "0 2\n",
      "262729\n",
      "0 6\n",
      "262786\n",
      "6 0\n",
      "262788\n",
      "4 2\n",
      "262796\n",
      "2 0\n",
      "262804\n",
      "2 0\n",
      "262807\n",
      "2 0\n",
      "262818\n",
      "4 0\n",
      "262830\n",
      "14 11\n",
      "262850\n",
      "6 0\n",
      "262854\n",
      "4 0\n",
      "262855\n",
      "6 2\n",
      "262864\n",
      "4 0\n",
      "262865\n",
      "6 2\n",
      "262866\n",
      "5 14\n",
      "262868\n",
      "0 6\n",
      "262870\n",
      "4 5\n",
      "262916\n",
      "0 2\n",
      "262917\n",
      "6 14\n",
      "262980\n",
      "10 12\n",
      "262981\n",
      "8 0\n",
      "262987\n",
      "8 10\n",
      "262988\n",
      "6 2\n",
      "263364\n",
      "4 0\n",
      "263496\n",
      "0 2\n",
      "263668\n",
      "4 5\n",
      "263694\n",
      "2 4\n",
      "263695\n",
      "2 4\n",
      "263986\n",
      "2 14\n",
      "267050\n",
      "4 0\n",
      "267127\n",
      "0 2\n",
      "267130\n",
      "4 0\n",
      "269619\n",
      "2 14\n",
      "276938\n",
      "0 2\n",
      "276944\n",
      "4 0\n",
      "276949\n",
      "0 6\n",
      "276985\n",
      "2 0\n",
      "276987\n",
      "8 0\n",
      "277012\n",
      "4 0\n",
      "277034\n",
      "0 10\n",
      "277035\n",
      "6 14\n",
      "277039\n",
      "0 6\n",
      "277040\n",
      "6 2\n",
      "277120\n",
      "0 2\n",
      "277143\n",
      "1 5\n",
      "277215\n",
      "2 0\n",
      "277217\n",
      "0 4\n",
      "277875\n",
      "0 2\n",
      "278506\n",
      "3 2\n",
      "278582\n",
      "3 2\n",
      "287343\n",
      "2 14\n",
      "295866\n",
      "4 0\n",
      "295894\n",
      "0 6\n",
      "295902\n",
      "2 0\n",
      "295904\n",
      "4 0\n",
      "295932\n",
      "2 0\n",
      "295967\n",
      "2 4\n",
      "296484\n",
      "0 2\n",
      "314720\n",
      "0 6\n",
      "314755\n",
      "0 2\n",
      "314762\n",
      "0 2\n",
      "314874\n",
      "0 2\n",
      "314876\n",
      "4 0\n",
      "317681\n",
      "3 2\n",
      "318552\n",
      "2 14\n",
      "321620\n",
      "4 2\n",
      "324863\n",
      "14 2\n",
      "324864\n",
      "14 2\n",
      "328044\n",
      "3 1\n",
      "329179\n",
      "0 2\n",
      "329980\n",
      "6 14\n",
      "330167\n",
      "8 0\n",
      "330168\n",
      "6 0\n",
      "330169\n",
      "0 6\n",
      "330227\n",
      "0 2\n",
      "330234\n",
      "2 0\n",
      "330236\n",
      "12 14\n",
      "330270\n",
      "0 14\n",
      "330311\n",
      "4 0\n",
      "330312\n",
      "0 6\n",
      "330374\n",
      "0 2\n",
      "330375\n",
      "6 14\n",
      "330381\n",
      "6 4\n",
      "335001\n",
      "0 2\n",
      "339311\n",
      "4 5\n",
      "339325\n",
      "2 0\n",
      "339336\n",
      "10 8\n",
      "339337\n",
      "14 6\n",
      "339357\n",
      "0 8\n",
      "348290\n",
      "6 2\n",
      "348297\n",
      "4 2\n",
      "348791\n",
      "2 14\n",
      "354009\n",
      "2 14\n",
      "354617\n",
      "0 4\n",
      "354618\n",
      "0 6\n",
      "354619\n",
      "0 3\n",
      "354622\n",
      "0 2\n",
      "355640\n",
      "0 14\n",
      "355656\n",
      "2 4\n",
      "355900\n",
      "0 4\n",
      "357936\n",
      "2 14\n",
      "358116\n",
      "14 0\n",
      "358455\n",
      "0 14\n",
      "359315\n",
      "2 8\n",
      "359375\n",
      "4 14\n",
      "361379\n",
      "14 12\n",
      "366385\n",
      "6 14\n",
      "366933\n",
      "0 4\n",
      "370726\n",
      "0 2\n",
      "370744\n",
      "0 2\n",
      "373711\n",
      "2 14\n",
      "376342\n",
      "4 0\n",
      "378434\n",
      "0 6\n",
      "378915\n",
      "2 0\n",
      "378988\n",
      "2 0\n",
      "379070\n",
      "1 0\n",
      "379119\n",
      "0 2\n",
      "384048\n",
      "0 2\n",
      "384098\n",
      "5 0\n",
      "384103\n",
      "2 0\n",
      "384504\n",
      "2 14\n",
      "385799\n",
      "4 6\n",
      "385870\n",
      "0 6\n",
      "385871\n",
      "12 2\n",
      "385885\n",
      "4 0\n",
      "385932\n",
      "0 14\n",
      "385936\n",
      "14 0\n",
      "385942\n",
      "12 8\n",
      "385943\n",
      "14 12\n",
      "392929\n",
      "0 6\n",
      "392930\n",
      "0 5\n",
      "392936\n",
      "0 2\n",
      "392960\n",
      "0 2\n",
      "392963\n",
      "2 0\n",
      "392964\n",
      "3 2\n",
      "392975\n",
      "3 5\n",
      "393050\n",
      "2 0\n",
      "393057\n",
      "2 0\n",
      "397487\n",
      "0 4\n",
      "397491\n",
      "2 4\n",
      "399232\n",
      "10 2\n",
      "399252\n",
      "5 0\n",
      "399256\n",
      "13 11\n",
      "399290\n",
      "0 6\n",
      "399308\n",
      "2 4\n",
      "400542\n",
      "3 2\n",
      "400708\n",
      "4 14\n",
      "401032\n",
      "6 2\n",
      "401150\n",
      "6 2\n",
      "401186\n",
      "2 0\n",
      "403411\n",
      "4 2\n",
      "403414\n",
      "0 4\n",
      "403524\n",
      "2 0\n",
      "407888\n",
      "10 8\n",
      "408515\n",
      "4 0\n",
      "408884\n",
      "0 2\n",
      "410097\n",
      "4 5\n",
      "410098\n",
      "0 4\n",
      "410104\n",
      "2 4\n",
      "410107\n",
      "4 0\n",
      "410108\n",
      "0 6\n",
      "418222\n",
      "4 2\n",
      "418747\n",
      "2 4\n",
      "419227\n",
      "14 0\n",
      "420864\n",
      "10 8\n",
      "420865\n",
      "4 5\n",
      "420877\n",
      "5 3\n",
      "421297\n",
      "0 6\n",
      "421298\n",
      "12 4\n",
      "421300\n",
      "3 1\n",
      "421303\n",
      "0 4\n",
      "423703\n",
      "0 2\n",
      "423707\n",
      "4 0\n",
      "423725\n",
      "0 2\n",
      "423729\n",
      "4 0\n",
      "423771\n",
      "0 2\n",
      "423775\n",
      "4 0\n",
      "423813\n",
      "0 2\n",
      "423817\n",
      "4 0\n",
      "423835\n",
      "3 2\n",
      "423867\n",
      "0 4\n",
      "423868\n",
      "6 4\n",
      "423881\n",
      "2 0\n",
      "423882\n",
      "0 6\n",
      "423883\n",
      "8 0\n",
      "423884\n",
      "0 4\n",
      "423914\n",
      "2 0\n",
      "425997\n",
      "0 2\n",
      "Accuracy: 99.88%\n"
     ]
    }
   ],
   "source": [
    "test_dataset = LettersDataset('clean_out/X_test_no_harakat.csv', 'clean_out/X_test_no_harakat.csv',val_mode=True, device=device)   \n",
    "\n",
    "val_loader = data.DataLoader(test_dataset,  batch_size=batch_size)\n",
    "print(test_dataset.char_encoder.word2idx)\n",
    "# evaluaate accuracy on validation set\n",
    "\n",
    "model.eval()\n",
    "letter_haraka = []\n",
    "with torch.no_grad():\n",
    "    for (X_batch,y_batch) in val_loader:\n",
    "        # y_pred = model(X_batch)['diacritics']\n",
    "        y_pred = model(X_batch)\n",
    "        # we transpose because the loss function expects the second dimension to be the classes\n",
    "        # y_pred is now (batch_size, n_classes, seq_len)\n",
    "        y_pred = y_pred.transpose(1, 2) \n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        # Count only non-padding characters\n",
    "        for x,y in zip(X_batch,predicted):\n",
    "            for xx,yy in zip(x,y):\n",
    "                # we reached the end of the sentence\n",
    "                # print(xx.item())\n",
    "                # print(test_dataset.char_encoder.get_pad_id())\n",
    "                # print(test_dataset.char_encoder.get_id_by_token(UNK_TOKEN))\n",
    "                if xx.item() == test_dataset.char_encoder.get_pad_id():\n",
    "                    break\n",
    "                ll = test_dataset.char_encoder.is_arabic_letter(xx.item())\n",
    "                if ll:\n",
    "                    letter_haraka.append([ll,yy.item()])\n",
    "\n",
    "# save ID,Label pairs in a csv file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(letter_haraka, columns=['letter','label'])\n",
    "df.to_csv('./results/letter_haraka.csv', index=True, index_label='ID')\n",
    "\n",
    "\n",
    "\n",
    "gold_test = pd.read_csv('clean_out/test_gold.csv',index_col=0)\n",
    "sys_test = pd.read_csv('results/letter_haraka.csv',index_col=0)\n",
    "# Accuracy per letter\n",
    "# print(gold_test.head())\n",
    "# print(sys_test.head())   \n",
    "# print(gold_test.iloc[0]['label'])\n",
    "\n",
    "correct = 0\n",
    "total = len(gold_test)\n",
    "for i in range(total):\n",
    "    # print(gold_test[i][0], sys_test[i][0])\n",
    "    if gold_test.iloc[i]['label'] == sys_test.iloc[i]['label']:\n",
    "        correct +=1\n",
    "    else:\n",
    "        print(i)\n",
    "        print(gold_test.iloc[i]['label'], sys_test.iloc[i]['label'])\n",
    "    \n",
    "print(\"Accuracy: %.2f%%\" % (100.0 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 1184\n",
      "{'ا': 0, 'ب': 1, 'ت': 2, 'ث': 3, 'ج': 4, 'ح': 5, 'خ': 6, 'د': 7, 'ذ': 8, 'ر': 9, 'ز': 10, 'س': 11, 'ش': 12, 'ص': 13, 'ض': 14, 'ط': 15, 'ظ': 16, 'ع': 17, 'غ': 18, 'ف': 19, 'ق': 20, 'ك': 21, 'ل': 22, 'م': 23, 'ن': 24, 'ه': 25, 'و': 26, 'ي': 27, 'ى': 28, 'ة': 29, 'آ': 30, 'أ': 31, 'إ': 32, 'ء': 33, 'ؤ': 34, 'ئ': 35, ' ': 36, '،': 37, '-': 38, '<pad>': 39, '<unk>': 40}\n"
     ]
    }
   ],
   "source": [
    "test_dataset = LettersDataset('clean_out/X_test_no_diacritics.csv', 'clean_out/Y_test_no_diacritics.csv',val_mode=True, device=device)   \n",
    "val_loader = data.DataLoader(test_dataset,  batch_size=batch_size)\n",
    "print(test_dataset.char_encoder.word2idx)\n",
    "# evaluaate accuracy on validation set\n",
    "\n",
    "model.eval()\n",
    "letter_haraka = []\n",
    "with torch.no_grad():\n",
    "    for (X_batch,y_batch) in val_loader:\n",
    "        # y_pred = model(X_batch)['diacritics']\n",
    "        y_pred = model(X_batch)\n",
    "        # we transpose because the loss function expects the second dimension to be the classes\n",
    "        # y_pred is now (batch_size, n_classes, seq_len)\n",
    "        y_pred = y_pred.transpose(1, 2) \n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        # Count only non-padding characters\n",
    "        for x,y in zip(X_batch,predicted):\n",
    "            for xx,yy in zip(x,y):\n",
    "                # we reached the end of the sentence\n",
    "                # print(xx.item())\n",
    "                # print(test_dataset.char_encoder.get_pad_id())\n",
    "                # print(test_dataset.char_encoder.get_id_by_token(UNK_TOKEN))\n",
    "                if xx.item() == test_dataset.char_encoder.get_pad_id():\n",
    "                    break\n",
    "                ll = test_dataset.char_encoder.is_arabic_letter(xx.item())\n",
    "                if ll:\n",
    "                    letter_haraka.append(yy.item())\n",
    "\n",
    "# save ID,Label pairs in a csv file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(letter_haraka, columns=['label'])\n",
    "df.to_csv('./results/letter_diacritic.csv', index=True, index_label='ID')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
