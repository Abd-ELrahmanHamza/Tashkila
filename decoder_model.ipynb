{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader  # these are needed for the training data\n",
    "import numpy as np\n",
    "from constants import *\n",
    "# import optim\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "from torch.optim import Adam # optim contains many optimizers. This time we're using Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        print(\"from decoder init\")\n",
    "        print(\"adham\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).cuda()\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size).cuda()\n",
    "        # print(\"from decoder forward\")\n",
    "        # print(x.shape)\n",
    "        embeddings = self.embedding(x).cuda()\n",
    "        # print(\"from decoder forward after embedding\")\n",
    "        # print(embeddings.shape)\n",
    "        h, (hn, cn) = self.rnn(embeddings, (h0, c0))\n",
    "        # h is the output of the RNN\n",
    "        # hn is the hidden state of the last timestep\n",
    "        # cn is the cell state of the last timestep\n",
    "        out = self.fc(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 495\n"
     ]
    }
   ],
   "source": [
    "from letters_dataset import LettersDataset\n",
    "from train_collections import DS_ARABIC_LETTERS, DS_HARAKAT\n",
    "\n",
    "\n",
    "dim_vocab = len(DS_ARABIC_LETTERS)\n",
    "dim_out = len(DS_HARAKAT) + 2\n",
    "embedding_dim = 64\n",
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = LettersDataset(device=device)\n",
    "loader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# load val data\n",
    "# da = LettersDataset('clean_out/X_val.csv', 'clean_out/y_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from decoder init\n",
      "adham\n",
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "# create a decoder\n",
    "model = Decoder(\n",
    "    input_size= dim_vocab,\n",
    "    hidden_size=256,\n",
    "    output_size=dim_out,\n",
    "    embedding_size=embedding_dim,\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "   print(\"cuda is available\")\n",
    "   model = model.cuda()\n",
    "#    x_train = x_train.cuda() \n",
    "#    y_train = y_train.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 2590\n",
      "Epoch 0, batch 0: Loss = 2.6960\n",
      "Epoch 0, batch 100: Loss = 0.1495\n",
      "Epoch 0, batch 200: Loss = 0.1011\n",
      "Epoch 0, batch 300: Loss = 0.0915\n",
      "Epoch 0, batch 400: Loss = 0.1044\n",
      "Epoch 0, batch 500: Loss = 0.0824\n",
      "Epoch 0, batch 600: Loss = 0.0879\n",
      "Epoch 0, batch 700: Loss = 0.0862\n",
      "Epoch 0, batch 800: Loss = 0.0788\n",
      "Epoch 0, batch 900: Loss = 0.0584\n",
      "Epoch 0, batch 1000: Loss = 0.0624\n",
      "Epoch 0, batch 1100: Loss = 0.0635\n",
      "Epoch 0, batch 1200: Loss = 0.0670\n",
      "Epoch 0, batch 1300: Loss = 0.1363\n",
      "Epoch 0, batch 1400: Loss = 0.0645\n",
      "Epoch 0, batch 1500: Loss = 0.0552\n",
      "Epoch 0, batch 1600: Loss = 0.0630\n",
      "Epoch 0, batch 1700: Loss = 0.0694\n",
      "Epoch 0, batch 1800: Loss = 0.0530\n",
      "Epoch 0, batch 1900: Loss = 0.0804\n",
      "Epoch 0, batch 2000: Loss = 0.0727\n",
      "Epoch 0, batch 2100: Loss = 0.0921\n",
      "Epoch 0, batch 2200: Loss = 0.0549\n",
      "Epoch 0, batch 2300: Loss = 0.0598\n",
      "Epoch 0, batch 2400: Loss = 0.0656\n",
      "Epoch 0, batch 2500: Loss = 0.0666\n",
      "Epoch 1, batch 0: Loss = 0.0589\n",
      "Epoch 1, batch 100: Loss = 0.0436\n",
      "Epoch 1, batch 200: Loss = 0.0619\n",
      "Epoch 1, batch 300: Loss = 0.0430\n",
      "Epoch 1, batch 400: Loss = 0.0667\n",
      "Epoch 1, batch 500: Loss = 0.0566\n",
      "Epoch 1, batch 600: Loss = 0.0432\n",
      "Epoch 1, batch 700: Loss = 0.0602\n",
      "Epoch 1, batch 800: Loss = 0.0552\n",
      "Epoch 1, batch 900: Loss = 0.0613\n",
      "Epoch 1, batch 1000: Loss = 0.0593\n",
      "Epoch 1, batch 1100: Loss = 0.0411\n",
      "Epoch 1, batch 1200: Loss = 0.0600\n",
      "Epoch 1, batch 1300: Loss = 0.0609\n",
      "Epoch 1, batch 1400: Loss = 0.0542\n",
      "Epoch 1, batch 1500: Loss = 0.0575\n",
      "Epoch 1, batch 1600: Loss = 0.0630\n",
      "Epoch 1, batch 1700: Loss = 0.0680\n",
      "Epoch 1, batch 1800: Loss = 0.0631\n",
      "Epoch 1, batch 1900: Loss = 0.0532\n",
      "Epoch 1, batch 2000: Loss = 0.0278\n",
      "Epoch 1, batch 2100: Loss = 0.0346\n",
      "Epoch 1, batch 2200: Loss = 0.0579\n",
      "Epoch 1, batch 2300: Loss = 0.0543\n",
      "Epoch 1, batch 2400: Loss = 0.0537\n",
      "Epoch 1, batch 2500: Loss = 0.0551\n",
      "Epoch 2, batch 0: Loss = 0.0616\n",
      "Epoch 2, batch 100: Loss = 0.0792\n",
      "Epoch 2, batch 200: Loss = 0.0460\n",
      "Epoch 2, batch 300: Loss = 0.0476\n",
      "Epoch 2, batch 400: Loss = 0.0523\n",
      "Epoch 2, batch 500: Loss = 0.0436\n",
      "Epoch 2, batch 600: Loss = 0.0478\n",
      "Epoch 2, batch 700: Loss = 0.0470\n",
      "Epoch 2, batch 800: Loss = 0.0658\n",
      "Epoch 2, batch 900: Loss = 0.0428\n",
      "Epoch 2, batch 1000: Loss = 0.0537\n",
      "Epoch 2, batch 1100: Loss = 0.0503\n",
      "Epoch 2, batch 1200: Loss = 0.0574\n",
      "Epoch 2, batch 1300: Loss = 0.0456\n",
      "Epoch 2, batch 1400: Loss = 0.0365\n",
      "Epoch 2, batch 1500: Loss = 0.0459\n",
      "Epoch 2, batch 1600: Loss = 0.0471\n",
      "Epoch 2, batch 1700: Loss = 0.0587\n",
      "Epoch 2, batch 1800: Loss = 0.0488\n",
      "Epoch 2, batch 1900: Loss = 0.0420\n",
      "Epoch 2, batch 2000: Loss = 0.0479\n",
      "Epoch 2, batch 2100: Loss = 0.0624\n",
      "Epoch 2, batch 2200: Loss = 0.0489\n",
      "Epoch 2, batch 2300: Loss = 0.0479\n",
      "Epoch 2, batch 2400: Loss = 0.0459\n",
      "Epoch 2, batch 2500: Loss = 0.0402\n",
      "Epoch 3, batch 0: Loss = 0.0502\n",
      "Epoch 3, batch 100: Loss = 0.0369\n",
      "Epoch 3, batch 200: Loss = 0.0517\n",
      "Epoch 3, batch 300: Loss = 0.0507\n",
      "Epoch 3, batch 400: Loss = 0.0395\n",
      "Epoch 3, batch 500: Loss = 0.0355\n",
      "Epoch 3, batch 600: Loss = 0.0636\n",
      "Epoch 3, batch 700: Loss = 0.0422\n",
      "Epoch 3, batch 800: Loss = 0.0532\n",
      "Epoch 3, batch 900: Loss = 0.0481\n",
      "Epoch 3, batch 1000: Loss = 0.0358\n",
      "Epoch 3, batch 1100: Loss = 0.0474\n",
      "Epoch 3, batch 1200: Loss = 0.0380\n",
      "Epoch 3, batch 1300: Loss = 0.0580\n",
      "Epoch 3, batch 1400: Loss = 0.0419\n",
      "Epoch 3, batch 1500: Loss = 0.0636\n",
      "Epoch 3, batch 1600: Loss = 0.0561\n",
      "Epoch 3, batch 1700: Loss = 0.0374\n",
      "Epoch 3, batch 1800: Loss = 0.0430\n",
      "Epoch 3, batch 1900: Loss = 0.0435\n",
      "Epoch 3, batch 2000: Loss = 0.0612\n",
      "Epoch 3, batch 2100: Loss = 0.0573\n",
      "Epoch 3, batch 2200: Loss = 0.0449\n",
      "Epoch 3, batch 2300: Loss = 0.0545\n",
      "Epoch 3, batch 2400: Loss = 0.0457\n",
      "Epoch 3, batch 2500: Loss = 0.0543\n",
      "Epoch 4, batch 0: Loss = 0.0591\n",
      "Epoch 4, batch 100: Loss = 0.0501\n",
      "Epoch 4, batch 200: Loss = 0.0377\n",
      "Epoch 4, batch 300: Loss = 0.0499\n",
      "Epoch 4, batch 400: Loss = 0.0454\n",
      "Epoch 4, batch 500: Loss = 0.0563\n",
      "Epoch 4, batch 600: Loss = 0.0400\n",
      "Epoch 4, batch 700: Loss = 0.0447\n",
      "Epoch 4, batch 800: Loss = 0.0475\n",
      "Epoch 4, batch 900: Loss = 0.0318\n",
      "Epoch 4, batch 1000: Loss = 0.0480\n",
      "Epoch 4, batch 1100: Loss = 0.0353\n",
      "Epoch 4, batch 1200: Loss = 0.0362\n",
      "Epoch 4, batch 1300: Loss = 0.0701\n",
      "Epoch 4, batch 1400: Loss = 0.0592\n",
      "Epoch 4, batch 1500: Loss = 0.0449\n",
      "Epoch 4, batch 1600: Loss = 0.0596\n",
      "Epoch 4, batch 1700: Loss = 0.0345\n",
      "Epoch 4, batch 1800: Loss = 0.0484\n",
      "Epoch 4, batch 1900: Loss = 0.0485\n",
      "Epoch 4, batch 2000: Loss = 0.0531\n",
      "Epoch 4, batch 2100: Loss = 0.0368\n",
      "Epoch 4, batch 2200: Loss = 0.0357\n",
      "Epoch 4, batch 2300: Loss = 0.0438\n",
      "Epoch 4, batch 2400: Loss = 0.0362\n",
      "Epoch 4, batch 2500: Loss = 0.0624\n",
      "Epoch 5, batch 0: Loss = 0.0459\n",
      "Epoch 5, batch 100: Loss = 0.0372\n",
      "Epoch 5, batch 200: Loss = 0.0555\n",
      "Epoch 5, batch 300: Loss = 0.0504\n",
      "Epoch 5, batch 400: Loss = 0.0484\n",
      "Epoch 5, batch 500: Loss = 0.0564\n",
      "Epoch 5, batch 600: Loss = 0.0397\n",
      "Epoch 5, batch 700: Loss = 0.0563\n",
      "Epoch 5, batch 800: Loss = 0.0507\n",
      "Epoch 5, batch 900: Loss = 0.0436\n",
      "Epoch 5, batch 1000: Loss = 0.0643\n",
      "Epoch 5, batch 1100: Loss = 0.0569\n",
      "Epoch 5, batch 1200: Loss = 0.0334\n",
      "Epoch 5, batch 1300: Loss = 0.0364\n",
      "Epoch 5, batch 1400: Loss = 0.0448\n",
      "Epoch 5, batch 1500: Loss = 0.0479\n",
      "Epoch 5, batch 1600: Loss = 0.0400\n",
      "Epoch 5, batch 1700: Loss = 0.0501\n",
      "Epoch 5, batch 1800: Loss = 0.0377\n",
      "Epoch 5, batch 1900: Loss = 0.0411\n",
      "Epoch 5, batch 2000: Loss = 0.0348\n",
      "Epoch 5, batch 2100: Loss = 0.0462\n",
      "Epoch 5, batch 2200: Loss = 0.0480\n",
      "Epoch 5, batch 2300: Loss = 0.0577\n",
      "Epoch 5, batch 2400: Loss = 0.0381\n",
      "Epoch 5, batch 2500: Loss = 0.0541\n",
      "Epoch 6, batch 0: Loss = 0.0385\n",
      "Epoch 6, batch 100: Loss = 0.0576\n",
      "Epoch 6, batch 200: Loss = 0.0314\n",
      "Epoch 6, batch 300: Loss = 0.0634\n",
      "Epoch 6, batch 400: Loss = 0.0494\n",
      "Epoch 6, batch 500: Loss = 0.0375\n",
      "Epoch 6, batch 600: Loss = 0.0621\n",
      "Epoch 6, batch 700: Loss = 0.0409\n",
      "Epoch 6, batch 800: Loss = 0.0490\n",
      "Epoch 6, batch 900: Loss = 0.0483\n",
      "Epoch 6, batch 1000: Loss = 0.0525\n",
      "Epoch 6, batch 1100: Loss = 0.0343\n",
      "Epoch 6, batch 1200: Loss = 0.0427\n",
      "Epoch 6, batch 1300: Loss = 0.0550\n",
      "Epoch 6, batch 1400: Loss = 0.0478\n",
      "Epoch 6, batch 1500: Loss = 0.0555\n",
      "Epoch 6, batch 1600: Loss = 0.0364\n",
      "Epoch 6, batch 1700: Loss = 0.0517\n",
      "Epoch 6, batch 1800: Loss = 0.0509\n",
      "Epoch 6, batch 1900: Loss = 0.0461\n",
      "Epoch 6, batch 2000: Loss = 0.0366\n",
      "Epoch 6, batch 2100: Loss = 0.0366\n",
      "Epoch 6, batch 2200: Loss = 0.0434\n",
      "Epoch 6, batch 2300: Loss = 0.0549\n",
      "Epoch 6, batch 2400: Loss = 0.0445\n",
      "Epoch 6, batch 2500: Loss = 0.0424\n",
      "Epoch 7, batch 0: Loss = 0.0458\n",
      "Epoch 7, batch 100: Loss = 0.0381\n",
      "Epoch 7, batch 200: Loss = 0.0446\n",
      "Epoch 7, batch 300: Loss = 0.0401\n",
      "Epoch 7, batch 400: Loss = 0.0451\n",
      "Epoch 7, batch 500: Loss = 0.0324\n",
      "Epoch 7, batch 600: Loss = 0.0345\n",
      "Epoch 7, batch 700: Loss = 0.0400\n",
      "Epoch 7, batch 800: Loss = 0.0506\n",
      "Epoch 7, batch 900: Loss = 0.0487\n",
      "Epoch 7, batch 1000: Loss = 0.0502\n",
      "Epoch 7, batch 1100: Loss = 0.0303\n",
      "Epoch 7, batch 1200: Loss = 0.0317\n",
      "Epoch 7, batch 1300: Loss = 0.0406\n",
      "Epoch 7, batch 1400: Loss = 0.0373\n",
      "Epoch 7, batch 1500: Loss = 0.0487\n",
      "Epoch 7, batch 1600: Loss = 0.0352\n",
      "Epoch 7, batch 1700: Loss = 0.0554\n",
      "Epoch 7, batch 1800: Loss = 0.0378\n",
      "Epoch 7, batch 1900: Loss = 0.0430\n",
      "Epoch 7, batch 2000: Loss = 0.0466\n",
      "Epoch 7, batch 2100: Loss = 0.0455\n",
      "Epoch 7, batch 2200: Loss = 0.0381\n",
      "Epoch 7, batch 2300: Loss = 0.0434\n",
      "Epoch 7, batch 2400: Loss = 0.0511\n",
      "Epoch 7, batch 2500: Loss = 0.0467\n",
      "Epoch 8, batch 0: Loss = 0.0426\n",
      "Epoch 8, batch 100: Loss = 0.0344\n",
      "Epoch 8, batch 200: Loss = 0.0405\n",
      "Epoch 8, batch 300: Loss = 0.0349\n",
      "Epoch 8, batch 400: Loss = 0.0278\n",
      "Epoch 8, batch 500: Loss = 0.0440\n",
      "Epoch 8, batch 600: Loss = 0.0507\n",
      "Epoch 8, batch 700: Loss = 0.0432\n",
      "Epoch 8, batch 800: Loss = 0.0371\n",
      "Epoch 8, batch 900: Loss = 0.0295\n",
      "Epoch 8, batch 1000: Loss = 0.0345\n",
      "Epoch 8, batch 1100: Loss = 0.0464\n",
      "Epoch 8, batch 1200: Loss = 0.0319\n",
      "Epoch 8, batch 1300: Loss = 0.0624\n",
      "Epoch 8, batch 1400: Loss = 0.0435\n",
      "Epoch 8, batch 1500: Loss = 0.0412\n",
      "Epoch 8, batch 1600: Loss = 0.0554\n",
      "Epoch 8, batch 1700: Loss = 0.0346\n",
      "Epoch 8, batch 1800: Loss = 0.0429\n",
      "Epoch 8, batch 1900: Loss = 0.0325\n",
      "Epoch 8, batch 2000: Loss = 0.0509\n",
      "Epoch 8, batch 2100: Loss = 0.0403\n",
      "Epoch 8, batch 2200: Loss = 0.0365\n",
      "Epoch 8, batch 2300: Loss = 0.0598\n",
      "Epoch 8, batch 2400: Loss = 0.0344\n",
      "Epoch 8, batch 2500: Loss = 0.0370\n",
      "Epoch 9, batch 0: Loss = 0.0440\n",
      "Epoch 9, batch 100: Loss = 0.0290\n",
      "Epoch 9, batch 200: Loss = 0.0591\n",
      "Epoch 9, batch 300: Loss = 0.0586\n",
      "Epoch 9, batch 400: Loss = 0.0481\n",
      "Epoch 9, batch 500: Loss = 0.0452\n",
      "Epoch 9, batch 600: Loss = 0.0407\n",
      "Epoch 9, batch 700: Loss = 0.0553\n",
      "Epoch 9, batch 800: Loss = 0.0449\n",
      "Epoch 9, batch 900: Loss = 0.0467\n",
      "Epoch 9, batch 1000: Loss = 0.0519\n",
      "Epoch 9, batch 1100: Loss = 0.0389\n",
      "Epoch 9, batch 1200: Loss = 0.0428\n",
      "Epoch 9, batch 1300: Loss = 0.0407\n",
      "Epoch 9, batch 1400: Loss = 0.0441\n",
      "Epoch 9, batch 1500: Loss = 0.0384\n",
      "Epoch 9, batch 1600: Loss = 0.0552\n",
      "Epoch 9, batch 1700: Loss = 0.0574\n",
      "Epoch 9, batch 1800: Loss = 0.0387\n",
      "Epoch 9, batch 1900: Loss = 0.0727\n",
      "Epoch 9, batch 2000: Loss = 0.0457\n",
      "Epoch 9, batch 2100: Loss = 0.0463\n",
      "Epoch 9, batch 2200: Loss = 0.0296\n",
      "Epoch 9, batch 2300: Loss = 0.0498\n",
      "Epoch 9, batch 2400: Loss = 0.0648\n",
      "Epoch 9, batch 2500: Loss = 0.0583\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_batches = len(loader)\n",
    "print(\"Number of batches:\", num_batches)\n",
    "best_model = None\n",
    "best_loss = np.inf\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for i, (X_decoder, Y_batch) in enumerate(loader):\n",
    "        y_pred = ''\n",
    "        y_pred = model(X_decoder)\n",
    "        y_pred = y_pred.transpose(1, 2)\n",
    "        loss = loss_fn(y_pred, Y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch %d, batch %d: Loss = %.4f\" % (epoch, i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 500\n"
     ]
    }
   ],
   "source": [
    "# load X_test and y_test\n",
    "X_TEST_PATH = f'clean_out/X_val.csv'\n",
    "Y_TEST_PATH = f'clean_out/y_val.csv'\n",
    "test_dataset_tensor = LettersDataset(input_data_file=X_TEST_PATH, output_data_file=Y_TEST_PATH, device=device)\n",
    "\n",
    "# create a data loader\n",
    "test_data_loader = DataLoader(test_dataset_tensor, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.87%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (X_decoder, Y_batch) in test_data_loader:\n",
    "        is_padding = (X_decoder == test_dataset_tensor.char_encoder.get_pad_token())\n",
    "        y_pred = model(X_decoder)\n",
    "        y_pred = y_pred.transpose(1, 2)\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        # Count only non-padding characters\n",
    "        total += torch.sum(~is_padding).item()\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct += torch.sum((predicted == Y_batch) & (~is_padding)).item()\n",
    "print(\"Accuracy: %.2f%%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    def __init__(self, train_data_path, test_data_path, device):\n",
    "        self.train_data_path = train_data_path\n",
    "        self.test_data_path = test_data_path\n",
    "        self.device = device\n",
    "        self.dataset = LettersDataset(input_data_file=self.train_data_path, output_data_file=self.test_data_path, device=self.device)\n",
    "        self.decoder = Decoder(\n",
    "            input_size= dim_vocab,\n",
    "            hidden_size=256,\n",
    "            output_size=dim_out,\n",
    "            embedding_size=embedding_dim,\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.decoder(X)\n",
    "        y_pred = y_pred.transpose(1, 2)\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        return predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
