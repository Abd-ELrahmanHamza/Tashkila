{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from letters_dataset import LettersDataset\n",
    "from words_dataset import WordsDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from train_collections import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=128, hidden_dim=256, num_layers=1, dropout_probability=0.1):\n",
    "        super().__init__()\n",
    "        # TODO: replace with one hot encoding\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, num_layers, dropout=dropout_probability, batch_first=True)\n",
    "\n",
    "        # Dropout layer to prevent over fitting (regularization)\n",
    "        # it randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.\n",
    "        self.dropout = nn.Dropout(dropout_probability)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs = [inputs len, batch size]\n",
    "        embeddings = self.dropout(self.embedding(inputs))\n",
    "\n",
    "        # embedded = [inputs len, batch size, emb dim]\n",
    "        outputs, (hidden, cell) = self.lstm_layer(embeddings)\n",
    "\n",
    "        # outputs = [inputs len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        # outputs are always from the top hidden layer\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h0, c0):\n",
    "        # print(\"from decoder forward\")\n",
    "        # print(x.shape)\n",
    "        embeddings = self.embedding(x)\n",
    "        # print(\"from decoder forward after embedding\")\n",
    "        # print(embeddings.shape)\n",
    "        outs, _ = self.lstm(embeddings, (h0, c0))\n",
    "        # h is the output of the RNN\n",
    "        # hn is the hidden state of the last timestep\n",
    "        # cn is the cell state of the last timestep\n",
    "        scores = self.fc(outs)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, encoder_inputs, decoder_inputs):\n",
    "        encoder_hidden, encoder_cell = self.encoder(encoder_inputs)\n",
    "        # insert start token at the beginning of the decoder inputs\n",
    "        decoder_output = self.decoder(\n",
    "            decoder_inputs, encoder_hidden, encoder_cell)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AppendStartDataset(Dataset):\n",
    "    def __init__(self, letters_dataset: LettersDataset):\n",
    "        self.letters_dataset = letters_dataset\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.letters_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        (X_enc,Y) = self.letters_dataset[idx]\n",
    "        \n",
    "        X_dec = None\n",
    "        # add start token to the beginning of the decoder input\n",
    "        start_token = torch.tensor([self.letters_dataset.harakat_encoder.get_id_by_token(START_TOKEN)]).to(device)\n",
    "        X_dec = torch.cat((start_token, Y[:-1]))\n",
    "        \n",
    "        return X_enc,X_dec,Y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 495\n"
     ]
    }
   ],
   "source": [
    "from train_collections import *\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "letters_dataset = LettersDataset(\n",
    "    \"./clean_out/X.csv\", \"./clean_out/Y.csv\", device=device, special_tokens=[PAD_TOKEN,UNK_TOKEN,START_TOKEN,END_TOKEN])\n",
    "seq2seqDataset = AppendStartDataset(letters_dataset)\n",
    "\n",
    "loader = DataLoader(seq2seqDataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 495])\n",
      "torch.Size([64, 495])\n",
      "torch.Size([64, 495])\n",
      "tensor([[ 0,  3,  1,  ..., 15, 15, 15],\n",
      "        [14,  0,  3,  ..., 15, 15, 15],\n",
      "        [14,  0, 14,  ..., 15, 15, 15],\n",
      "        ...,\n",
      "        [14,  2,  3,  ..., 15, 15, 15],\n",
      "        [14,  0,  8,  ..., 15, 15, 15],\n",
      "        [14,  0,  3,  ..., 15, 15, 15]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(loader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)\n",
    "print(sample[2].shape)\n",
    "print(sample[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_chars = seq2seqDataset.letters_dataset.get_input_vocab_size()\n",
    "n_harakat = seq2seqDataset.letters_dataset.get_output_vocab_size()\n",
    "# encoder_dim_vocab = #tokens\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 265\n",
    "num_batches = len(loader)\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(64, 128)\n",
      "    (lstm_layer): LSTM(128, 128, batch_first=True)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(19, 128)\n",
      "    (lstm): LSTM(128, 128, batch_first=True)\n",
      "    (fc): Linear(in_features=128, out_features=19, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "enc_model = Encoder(\n",
    "    n_chars, hidden_dim=hidden_dim, num_layers=1, dropout_probability=0)\n",
    "\n",
    "dec_model = Decoder(n_harakat, embedding_size=128,\n",
    "                    hidden_size=128, output_size=n_harakat, device=device.type)\n",
    "\n",
    "\n",
    "model = Seq2Seq(encoder=enc_model, decoder=dec_model).to(device)\n",
    "print(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 2590\n",
      "Epoch 0, batch 0: Loss = 0.1520\n",
      "Epoch 0, batch 100: Loss = 0.1935\n",
      "Epoch 0, batch 200: Loss = 0.0951\n",
      "Epoch 0, batch 300: Loss = 0.1827\n",
      "Epoch 0, batch 400: Loss = 0.1850\n",
      "Epoch 0, batch 500: Loss = 0.1220\n",
      "Epoch 0, batch 600: Loss = 0.2171\n",
      "Epoch 0, batch 700: Loss = 0.1047\n",
      "Epoch 0, batch 800: Loss = 0.0954\n",
      "Epoch 0, batch 900: Loss = 0.0957\n",
      "Epoch 0, batch 1000: Loss = 0.1243\n",
      "Epoch 0, batch 1100: Loss = 0.1355\n",
      "Epoch 0, batch 1200: Loss = 0.1530\n",
      "Epoch 0, batch 1300: Loss = 0.0983\n",
      "Epoch 0, batch 1400: Loss = 0.1152\n",
      "Epoch 0, batch 1500: Loss = 0.1214\n",
      "Epoch 0, batch 1600: Loss = 0.1689\n",
      "Epoch 0, batch 1700: Loss = 0.0885\n",
      "Epoch 0, batch 1800: Loss = 0.1492\n",
      "Epoch 0, batch 1900: Loss = 0.1890\n",
      "Epoch 0, batch 2000: Loss = 0.0853\n",
      "Epoch 0, batch 2100: Loss = 0.1730\n",
      "Epoch 0, batch 2200: Loss = 0.1194\n",
      "Epoch 0, batch 2300: Loss = 0.2258\n",
      "Epoch 0, batch 2400: Loss = 0.1607\n",
      "Epoch 0, batch 2500: Loss = 0.1681\n",
      "Epoch 0: Cross-entropy: 376.1655\n",
      "Epoch 1, batch 0: Loss = 0.1440\n",
      "Epoch 1, batch 100: Loss = 0.1845\n",
      "Epoch 1, batch 200: Loss = 0.0889\n",
      "Epoch 1, batch 300: Loss = 0.1742\n",
      "Epoch 1, batch 400: Loss = 0.1756\n",
      "Epoch 1, batch 500: Loss = 0.1166\n",
      "Epoch 1, batch 600: Loss = 0.2096\n",
      "Epoch 1, batch 700: Loss = 0.0995\n",
      "Epoch 1, batch 800: Loss = 0.0901\n",
      "Epoch 1, batch 900: Loss = 0.0919\n",
      "Epoch 1, batch 1000: Loss = 0.1200\n",
      "Epoch 1, batch 1100: Loss = 0.1308\n",
      "Epoch 1, batch 1200: Loss = 0.1472\n",
      "Epoch 1, batch 1300: Loss = 0.0939\n",
      "Epoch 1, batch 1400: Loss = 0.1103\n",
      "Epoch 1, batch 1500: Loss = 0.1168\n",
      "Epoch 1, batch 1600: Loss = 0.1636\n",
      "Epoch 1, batch 1700: Loss = 0.0842\n",
      "Epoch 1, batch 1800: Loss = 0.1451\n",
      "Epoch 1, batch 1900: Loss = 0.1821\n",
      "Epoch 1, batch 2000: Loss = 0.0822\n",
      "Epoch 1, batch 2100: Loss = 0.1678\n",
      "Epoch 1, batch 2200: Loss = 0.1160\n",
      "Epoch 1, batch 2300: Loss = 0.2202\n",
      "Epoch 1, batch 2400: Loss = 0.1560\n",
      "Epoch 1, batch 2500: Loss = 0.1644\n",
      "Epoch 1: Cross-entropy: 366.0145\n",
      "Epoch 2, batch 0: Loss = 0.1405\n",
      "Epoch 2, batch 100: Loss = 0.1801\n",
      "Epoch 2, batch 200: Loss = 0.0859\n",
      "Epoch 2, batch 300: Loss = 0.1695\n",
      "Epoch 2, batch 400: Loss = 0.1716\n",
      "Epoch 2, batch 500: Loss = 0.1138\n",
      "Epoch 2, batch 600: Loss = 0.2054\n",
      "Epoch 2, batch 700: Loss = 0.0962\n",
      "Epoch 2, batch 800: Loss = 0.0870\n",
      "Epoch 2, batch 900: Loss = 0.0889\n",
      "Epoch 2, batch 1000: Loss = 0.1177\n",
      "Epoch 2, batch 1100: Loss = 0.1284\n",
      "Epoch 2, batch 1200: Loss = 0.1444\n",
      "Epoch 2, batch 1300: Loss = 0.0914\n",
      "Epoch 2, batch 1400: Loss = 0.1077\n",
      "Epoch 2, batch 1500: Loss = 0.1148\n",
      "Epoch 2, batch 1600: Loss = 0.1607\n",
      "Epoch 2, batch 1700: Loss = 0.0815\n",
      "Epoch 2, batch 1800: Loss = 0.1430\n",
      "Epoch 2, batch 1900: Loss = 0.1780\n",
      "Epoch 2, batch 2000: Loss = 0.0801\n",
      "Epoch 2, batch 2100: Loss = 0.1651\n",
      "Epoch 2, batch 2200: Loss = 0.1137\n",
      "Epoch 2, batch 2300: Loss = 0.2167\n",
      "Epoch 2, batch 2400: Loss = 0.1542\n",
      "Epoch 2, batch 2500: Loss = 0.1616\n",
      "Epoch 2: Cross-entropy: 360.3877\n",
      "Epoch 3, batch 0: Loss = 0.1388\n",
      "Epoch 3, batch 100: Loss = 0.1767\n",
      "Epoch 3, batch 200: Loss = 0.0845\n",
      "Epoch 3, batch 300: Loss = 0.1665\n",
      "Epoch 3, batch 400: Loss = 0.1689\n",
      "Epoch 3, batch 500: Loss = 0.1127\n",
      "Epoch 3, batch 600: Loss = 0.2033\n",
      "Epoch 3, batch 700: Loss = 0.0949\n",
      "Epoch 3, batch 800: Loss = 0.0854\n",
      "Epoch 3, batch 900: Loss = 0.0872\n",
      "Epoch 3, batch 1000: Loss = 0.1167\n",
      "Epoch 3, batch 1100: Loss = 0.1265\n",
      "Epoch 3, batch 1200: Loss = 0.1427\n",
      "Epoch 3, batch 1300: Loss = 0.0898\n",
      "Epoch 3, batch 1400: Loss = 0.1062\n",
      "Epoch 3, batch 1500: Loss = 0.1135\n",
      "Epoch 3, batch 1600: Loss = 0.1591\n",
      "Epoch 3, batch 1700: Loss = 0.0793\n",
      "Epoch 3, batch 1800: Loss = 0.1412\n",
      "Epoch 3, batch 1900: Loss = 0.1754\n",
      "Epoch 3, batch 2000: Loss = 0.0787\n",
      "Epoch 3, batch 2100: Loss = 0.1637\n",
      "Epoch 3, batch 2200: Loss = 0.1119\n",
      "Epoch 3, batch 2300: Loss = 0.2142\n",
      "Epoch 3, batch 2400: Loss = 0.1532\n",
      "Epoch 3, batch 2500: Loss = 0.1594\n",
      "Epoch 3: Cross-entropy: 355.1975\n",
      "Epoch 4, batch 0: Loss = 0.1370\n",
      "Epoch 4, batch 100: Loss = 0.1738\n",
      "Epoch 4, batch 200: Loss = 0.0835\n",
      "Epoch 4, batch 300: Loss = 0.1649\n",
      "Epoch 4, batch 400: Loss = 0.1670\n",
      "Epoch 4, batch 500: Loss = 0.1115\n",
      "Epoch 4, batch 600: Loss = 0.2014\n",
      "Epoch 4, batch 700: Loss = 0.0939\n",
      "Epoch 4, batch 800: Loss = 0.0842\n",
      "Epoch 4, batch 900: Loss = 0.0861\n",
      "Epoch 4, batch 1000: Loss = 0.1155\n",
      "Epoch 4, batch 1100: Loss = 0.1256\n",
      "Epoch 4, batch 1200: Loss = 0.1413\n",
      "Epoch 4, batch 1300: Loss = 0.0889\n",
      "Epoch 4, batch 1400: Loss = 0.1050\n",
      "Epoch 4, batch 1500: Loss = 0.1120\n",
      "Epoch 4, batch 1600: Loss = 0.1578\n",
      "Epoch 4, batch 1700: Loss = 0.0772\n",
      "Epoch 4, batch 1800: Loss = 0.1401\n",
      "Epoch 4, batch 1900: Loss = 0.1740\n",
      "Epoch 4, batch 2000: Loss = 0.0779\n",
      "Epoch 4, batch 2100: Loss = 0.1619\n",
      "Epoch 4, batch 2200: Loss = 0.1109\n",
      "Epoch 4, batch 2300: Loss = 0.2125\n",
      "Epoch 4, batch 2400: Loss = 0.1523\n",
      "Epoch 4, batch 2500: Loss = 0.1581\n",
      "Epoch 4: Cross-entropy: 351.7046\n",
      "Epoch 5, batch 0: Loss = 0.1353\n",
      "Epoch 5, batch 100: Loss = 0.1716\n",
      "Epoch 5, batch 200: Loss = 0.0827\n",
      "Epoch 5, batch 300: Loss = 0.1635\n",
      "Epoch 5, batch 400: Loss = 0.1654\n",
      "Epoch 5, batch 500: Loss = 0.1104\n",
      "Epoch 5, batch 600: Loss = 0.2002\n",
      "Epoch 5, batch 700: Loss = 0.0926\n",
      "Epoch 5, batch 800: Loss = 0.0834\n",
      "Epoch 5, batch 900: Loss = 0.0850\n",
      "Epoch 5, batch 1000: Loss = 0.1145\n",
      "Epoch 5, batch 1100: Loss = 0.1244\n",
      "Epoch 5, batch 1200: Loss = 0.1396\n",
      "Epoch 5, batch 1300: Loss = 0.0882\n",
      "Epoch 5, batch 1400: Loss = 0.1041\n",
      "Epoch 5, batch 1500: Loss = 0.1113\n",
      "Epoch 5, batch 1600: Loss = 0.1571\n",
      "Epoch 5, batch 1700: Loss = 0.0761\n",
      "Epoch 5, batch 1800: Loss = 0.1392\n",
      "Epoch 5, batch 1900: Loss = 0.1717\n",
      "Epoch 5, batch 2000: Loss = 0.0773\n",
      "Epoch 5, batch 2100: Loss = 0.1600\n",
      "Epoch 5, batch 2200: Loss = 0.1101\n",
      "Epoch 5, batch 2300: Loss = 0.2112\n",
      "Epoch 5, batch 2400: Loss = 0.1516\n",
      "Epoch 5, batch 2500: Loss = 0.1569\n",
      "Epoch 5: Cross-entropy: 349.2192\n",
      "Epoch 6, batch 0: Loss = 0.1342\n",
      "Epoch 6, batch 100: Loss = 0.1703\n",
      "Epoch 6, batch 200: Loss = 0.0823\n",
      "Epoch 6, batch 300: Loss = 0.1621\n",
      "Epoch 6, batch 400: Loss = 0.1640\n",
      "Epoch 6, batch 500: Loss = 0.1095\n",
      "Epoch 6, batch 600: Loss = 0.1996\n",
      "Epoch 6, batch 700: Loss = 0.0919\n",
      "Epoch 6, batch 800: Loss = 0.0827\n",
      "Epoch 6, batch 900: Loss = 0.0842\n",
      "Epoch 6, batch 1000: Loss = 0.1139\n",
      "Epoch 6, batch 1100: Loss = 0.1236\n",
      "Epoch 6, batch 1200: Loss = 0.1385\n",
      "Epoch 6, batch 1300: Loss = 0.0875\n",
      "Epoch 6, batch 1400: Loss = 0.1033\n",
      "Epoch 6, batch 1500: Loss = 0.1106\n",
      "Epoch 6, batch 1600: Loss = 0.1565\n",
      "Epoch 6, batch 1700: Loss = 0.0754\n",
      "Epoch 6, batch 1800: Loss = 0.1386\n",
      "Epoch 6, batch 1900: Loss = 0.1705\n",
      "Epoch 6, batch 2000: Loss = 0.0768\n",
      "Epoch 6, batch 2100: Loss = 0.1588\n",
      "Epoch 6, batch 2200: Loss = 0.1097\n",
      "Epoch 6, batch 2300: Loss = 0.2101\n",
      "Epoch 6, batch 2400: Loss = 0.1513\n",
      "Epoch 6, batch 2500: Loss = 0.1561\n",
      "Epoch 6: Cross-entropy: 347.2446\n",
      "Epoch 7, batch 0: Loss = 0.1337\n",
      "Epoch 7, batch 100: Loss = 0.1691\n",
      "Epoch 7, batch 200: Loss = 0.0819\n",
      "Epoch 7, batch 300: Loss = 0.1612\n",
      "Epoch 7, batch 400: Loss = 0.1630\n",
      "Epoch 7, batch 500: Loss = 0.1090\n",
      "Epoch 7, batch 600: Loss = 0.1991\n",
      "Epoch 7, batch 700: Loss = 0.0913\n",
      "Epoch 7, batch 800: Loss = 0.0821\n",
      "Epoch 7, batch 900: Loss = 0.0836\n",
      "Epoch 7, batch 1000: Loss = 0.1131\n",
      "Epoch 7, batch 1100: Loss = 0.1228\n",
      "Epoch 7, batch 1200: Loss = 0.1378\n",
      "Epoch 7, batch 1300: Loss = 0.0868\n",
      "Epoch 7, batch 1400: Loss = 0.1027\n",
      "Epoch 7, batch 1500: Loss = 0.1097\n",
      "Epoch 7, batch 1600: Loss = 0.1557\n",
      "Epoch 7, batch 1700: Loss = 0.0751\n",
      "Epoch 7, batch 1800: Loss = 0.1382\n",
      "Epoch 7, batch 1900: Loss = 0.1696\n",
      "Epoch 7, batch 2000: Loss = 0.0761\n",
      "Epoch 7, batch 2100: Loss = 0.1579\n",
      "Epoch 7, batch 2200: Loss = 0.1088\n",
      "Epoch 7, batch 2300: Loss = 0.2092\n",
      "Epoch 7, batch 2400: Loss = 0.1502\n",
      "Epoch 7, batch 2500: Loss = 0.1553\n",
      "Epoch 7: Cross-entropy: 345.5609\n",
      "Epoch 8, batch 0: Loss = 0.1330\n",
      "Epoch 8, batch 100: Loss = 0.1682\n",
      "Epoch 8, batch 200: Loss = 0.0815\n",
      "Epoch 8, batch 300: Loss = 0.1603\n",
      "Epoch 8, batch 400: Loss = 0.1621\n",
      "Epoch 8, batch 500: Loss = 0.1087\n",
      "Epoch 8, batch 600: Loss = 0.1988\n",
      "Epoch 8, batch 700: Loss = 0.0908\n",
      "Epoch 8, batch 800: Loss = 0.0816\n",
      "Epoch 8, batch 900: Loss = 0.0834\n",
      "Epoch 8, batch 1000: Loss = 0.1126\n",
      "Epoch 8, batch 1100: Loss = 0.1222\n",
      "Epoch 8, batch 1200: Loss = 0.1371\n",
      "Epoch 8, batch 1300: Loss = 0.0861\n",
      "Epoch 8, batch 1400: Loss = 0.1024\n",
      "Epoch 8, batch 1500: Loss = 0.1092\n",
      "Epoch 8, batch 1600: Loss = 0.1553\n",
      "Epoch 8, batch 1700: Loss = 0.0746\n",
      "Epoch 8, batch 1800: Loss = 0.1377\n",
      "Epoch 8, batch 1900: Loss = 0.1688\n",
      "Epoch 8, batch 2000: Loss = 0.0758\n",
      "Epoch 8, batch 2100: Loss = 0.1570\n",
      "Epoch 8, batch 2200: Loss = 0.1081\n",
      "Epoch 8, batch 2300: Loss = 0.2085\n",
      "Epoch 8, batch 2400: Loss = 0.1499\n",
      "Epoch 8, batch 2500: Loss = 0.1545\n",
      "Epoch 8: Cross-entropy: 344.1367\n",
      "Epoch 9, batch 0: Loss = 0.1329\n",
      "Epoch 9, batch 100: Loss = 0.1675\n",
      "Epoch 9, batch 200: Loss = 0.0812\n",
      "Epoch 9, batch 300: Loss = 0.1600\n",
      "Epoch 9, batch 400: Loss = 0.1613\n",
      "Epoch 9, batch 500: Loss = 0.1086\n",
      "Epoch 9, batch 600: Loss = 0.1982\n",
      "Epoch 9, batch 700: Loss = 0.0902\n",
      "Epoch 9, batch 800: Loss = 0.0813\n",
      "Epoch 9, batch 900: Loss = 0.0827\n",
      "Epoch 9, batch 1000: Loss = 0.1121\n",
      "Epoch 9, batch 1100: Loss = 0.1218\n",
      "Epoch 9, batch 1200: Loss = 0.1364\n",
      "Epoch 9, batch 1300: Loss = 0.0860\n",
      "Epoch 9, batch 1400: Loss = 0.1017\n",
      "Epoch 9, batch 1500: Loss = 0.1087\n",
      "Epoch 9, batch 1600: Loss = 0.1547\n",
      "Epoch 9, batch 1700: Loss = 0.0740\n",
      "Epoch 9, batch 1800: Loss = 0.1371\n",
      "Epoch 9, batch 1900: Loss = 0.1681\n",
      "Epoch 9, batch 2000: Loss = 0.0752\n",
      "Epoch 9, batch 2100: Loss = 0.1563\n",
      "Epoch 9, batch 2200: Loss = 0.1076\n",
      "Epoch 9, batch 2300: Loss = 0.2075\n",
      "Epoch 9, batch 2400: Loss = 0.1494\n",
      "Epoch 9, batch 2500: Loss = 0.1539\n",
      "Epoch 9: Cross-entropy: 342.8782\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of batches:\", num_batches)\n",
    "best_model = None\n",
    "best_loss = np.inf\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for i, (X_enc,X_dec, Y_batch) in enumerate(loader):\n",
    "        y_pred = ''\n",
    "        curr_batch_size = X_enc.shape[0]\n",
    "        y_pred = model(X_enc, X_dec)\n",
    "        y_pred = y_pred.transpose(1, 2)\n",
    "        # print(y_pred.shape)\n",
    "        # print(y_batch.shape)\n",
    "        loss = loss_fn(y_pred, Y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch %d, batch %d: Loss = %.4f\" % (epoch, i, loss))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (X_enc,X_dec, Y_batch) in loader:\n",
    "            y_pred = model(X_enc, X_dec)\n",
    "            y_pred = y_pred.transpose(1, 2)\n",
    "\n",
    "            loss += loss_fn(y_pred, Y_batch)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = model.state_dict()\n",
    "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 500\n",
      "Accuracy: 60.36%\n"
     ]
    }
   ],
   "source": [
    "val_dataset = LettersDataset(\n",
    "    'clean_out/X_val.csv', 'clean_out/y_val.csv', device=device, special_tokens=[PAD_TOKEN,UNK_TOKEN,START_TOKEN,END_TOKEN])\n",
    "val_dataset = AppendStartDataset(val_dataset)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (X_enc,X_dec, Y_batch) in val_loader:\n",
    "        is_padding = (X_enc == val_dataset.letters_dataset.char_encoder.get_pad_id())\n",
    "        y_pred = model(X_enc, X_dec)\n",
    "        y_pred = y_pred.transpose(1, 2)\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        # Count only non-padding characters\n",
    "        total += torch.sum(~is_padding).item()\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct += torch.sum((predicted == Y_batch) & (~is_padding)).item()\n",
    "print(\"Accuracy: %.2f%%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
